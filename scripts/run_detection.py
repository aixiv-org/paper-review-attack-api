#!/usr/bin/env python3
"""
æ£€æµ‹è„šæœ¬ (å¢å¼ºç‰ˆ)
å¯¹PDFæ–‡ä»¶è¿è¡Œæç¤ºè¯æ³¨å…¥æ£€æµ‹ï¼ŒåŒ…å«æ€§èƒ½æŒ‡æ ‡è®¡ç®—
"""

import sys
import os
import argparse
from pathlib import Path
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import List, Dict, Optional
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.detector import PromptInjectionDetector, EnsembleDetector
from src.utils import setup_logging, load_config, ProgressTracker, save_results

# å°è¯•å¯¼å…¥sklearnï¼Œå¦‚æœæ²¡æœ‰åˆ™è·³è¿‡æ€§èƒ½æŒ‡æ ‡
try:
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("è­¦å‘Š: sklearnæœªå®‰è£…ï¼Œå°†è·³è¿‡æ€§èƒ½æŒ‡æ ‡è®¡ç®—")

def calculate_performance_metrics(results: List[Dict], ground_truth: Optional[Dict] = None, 
                                threshold: Optional[float] = None) -> Dict:
    """è®¡ç®—æ€§èƒ½æŒ‡æ ‡ - ä¿®å¤ç‰ˆ"""
    if not results:
        return {}
    
    # åŸºç¡€ç»Ÿè®¡
    total_files = len(results)
    malicious_files = sum(1 for r in results if r['is_malicious'])
    risk_scores = [r['risk_score'] for r in results]
    detection_counts = [r['detection_count'] for r in results]
    
    basic_metrics = {
        'total_files': total_files,
        'malicious_files': malicious_files,
        'benign_files': total_files - malicious_files,
        'malicious_rate': malicious_files / total_files if total_files > 0 else 0,
        'avg_risk_score': np.mean(risk_scores) if risk_scores else 0,
        'std_risk_score': np.std(risk_scores) if risk_scores else 0,
        'min_risk_score': np.min(risk_scores) if risk_scores else 0,
        'max_risk_score': np.max(risk_scores) if risk_scores else 0,
        'median_risk_score': np.median(risk_scores) if risk_scores else 0,
        'avg_detection_count': np.mean(detection_counts) if detection_counts else 0,
        'threshold_used': threshold or 0.45
    }
    
    # é£é™©åˆ†å¸ƒ
    basic_metrics['risk_distribution'] = {
        'very_low': sum(1 for r in risk_scores if r < 0.1),
        'low': sum(1 for r in risk_scores if 0.1 <= r < 0.3),
        'medium': sum(1 for r in risk_scores if 0.3 <= r < 0.5),
        'high': sum(1 for r in risk_scores if 0.5 <= r < 0.7),
        'very_high': sum(1 for r in risk_scores if r >= 0.7)
    }
    
    # æ£€æµ‹ç±»å‹ç»Ÿè®¡
    detection_type_counts = {}
    total_detections = 0
    
    for result in results:
        for detection in result.get('detections', []):
            det_type = detection.get('type', 'unknown')
            detection_type_counts[det_type] = detection_type_counts.get(det_type, 0) + 1
            total_detections += 1
    
    basic_metrics['detection_statistics'] = {
        'total_detections': total_detections,
        'unique_detection_types': len(detection_type_counts),
        'type_distribution': detection_type_counts
    }
    
    # ğŸš€ ä¿®å¤ï¼šå¦‚æœæœ‰çœŸå®æ ‡ç­¾ä¸”sklearnå¯ç”¨ï¼Œè®¡ç®—æ€§èƒ½æŒ‡æ ‡
    if ground_truth and SKLEARN_AVAILABLE:
        print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯:")
        print(f"æ ‡ç­¾æ–‡ä»¶åŒ…å« {len(ground_truth)} ä¸ªæ¡ç›®")
        print(f"å‰5ä¸ªæ ‡ç­¾é”®: {list(ground_truth.keys())[:5]}")
        
        try:
            # å‡†å¤‡æ ‡ç­¾æ•°æ®
            y_true = []
            y_pred = []
            y_score = []
            matched_files = 0
            unmatched_files = []
            
            for result in results:
                file_path = result['file']
                file_name = Path(file_path).name
                file_stem = Path(file_path).stem
                
                # ğŸš€ å¢å¼ºï¼šå°è¯•æ›´å¤šåŒ¹é…æ¨¡å¼
                possible_keys = [
                    file_path,           # å®Œæ•´è·¯å¾„
                    file_name,           # æ–‡ä»¶å
                    file_stem,           # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
                    file_path.replace('\\', '/'),  # æ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦
                    str(Path(file_path).as_posix()),  # POSIXè·¯å¾„æ ¼å¼
                ]
                
                # ğŸš€ æ–°å¢ï¼šæ¨¡ç³ŠåŒ¹é…
                true_label = None
                matched_key = None
                
                # ç²¾ç¡®åŒ¹é…
                for key in possible_keys:
                    if key in ground_truth:
                        true_label = ground_truth[key]
                        matched_key = key
                        break
                
                # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…
                if true_label is None:
                    for gt_key in ground_truth.keys():
                        # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«åœ¨æ ‡ç­¾é”®ä¸­ï¼Œæˆ–åä¹‹
                        if (file_name in gt_key or gt_key in file_name or
                            file_stem in gt_key or gt_key in file_stem):
                            true_label = ground_truth[gt_key]
                            matched_key = gt_key
                            break
                
                if true_label is not None:
                    y_true.append(int(true_label))
                    y_pred.append(1 if result['is_malicious'] else 0)
                    y_score.append(result['risk_score'])
                    matched_files += 1
                    print(f"âœ… åŒ¹é…: {file_name} -> {matched_key} (æ ‡ç­¾: {true_label})")
                else:
                    unmatched_files.append(file_name)
                    print(f"âŒ æœªåŒ¹é…: {file_name}")
            
            print(f"\nğŸ“Š åŒ¹é…ç»“æœ: {matched_files}/{total_files} ä¸ªæ–‡ä»¶åŒ¹é…æˆåŠŸ")
            if unmatched_files:
                print(f"æœªåŒ¹é…æ–‡ä»¶: {unmatched_files[:3]}...")
            
            if matched_files > 0:
                basic_metrics['ground_truth_matched'] = matched_files
                basic_metrics['ground_truth_coverage'] = matched_files / total_files
                
                # ğŸš€ ä¿®å¤ï¼šç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                if len(set(y_true)) > 1 and len(y_true) > 0:  # ç¡®ä¿æœ‰æ­£è´Ÿæ ·æœ¬
                    try:
                        basic_metrics['performance'] = {
                            'accuracy': float(accuracy_score(y_true, y_pred)),
                            'precision': float(precision_score(y_true, y_pred, zero_division=0)),
                            'recall': float(recall_score(y_true, y_pred, zero_division=0)),
                            'f1_score': float(f1_score(y_true, y_pred, zero_division=0)),
                        }
                        
                        # è®¡ç®—æ··æ·†çŸ©é˜µ
                        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
                        basic_metrics['confusion_matrix'] = {
                            'true_negatives': int(tn),
                            'false_positives': int(fp),
                            'false_negatives': int(fn),
                            'true_positives': int(tp)
                        }
                        
                        # è®¡ç®—é¢å¤–æŒ‡æ ‡
                        basic_metrics['performance']['specificity'] = float(tn / (tn + fp)) if (tn + fp) > 0 else 0.0
                        basic_metrics['performance']['false_positive_rate'] = float(fp / (fp + tn)) if (fp + tn) > 0 else 0.0
                        basic_metrics['performance']['false_negative_rate'] = float(fn / (fn + tp)) if (fn + tp) > 0 else 0.0
                        
                        # è®¡ç®—ROC AUC
                        try:
                            basic_metrics['performance']['roc_auc'] = float(roc_auc_score(y_true, y_score))
                        except ValueError as e:
                            print(f"ROC AUCè®¡ç®—å¤±è´¥: {e}")
                            basic_metrics['performance']['roc_auc'] = 0.0
                        
                        print(f"âœ… æ€§èƒ½æŒ‡æ ‡è®¡ç®—æˆåŠŸ!")
                        
                    except Exception as e:
                        print(f"âŒ æ€§èƒ½æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
                        basic_metrics['performance'] = {'error': f'æ€§èƒ½æŒ‡æ ‡è®¡ç®—å¤±è´¥: {str(e)}'}
                        
                elif len(set(y_true)) <= 1:
                    basic_metrics['performance'] = {'note': f'åªæœ‰å•ä¸€ç±»åˆ« (ç±»åˆ«: {set(y_true)})ï¼Œæ— æ³•è®¡ç®—æ€§èƒ½æŒ‡æ ‡'}
                    print(f"âš ï¸ åªæœ‰å•ä¸€ç±»åˆ«: {set(y_true)}")
                    
            else:
                basic_metrics['performance'] = {'note': 'æ— åŒ¹é…çš„çœŸå®æ ‡ç­¾'}
                print("âŒ æ²¡æœ‰æˆåŠŸåŒ¹é…ä»»ä½•æ–‡ä»¶")
                
        except Exception as e:
            print(f"âŒ æ€§èƒ½æŒ‡æ ‡è®¡ç®—è¿‡ç¨‹å¤±è´¥: {e}")
            basic_metrics['performance'] = {'error': f'æ€§èƒ½æŒ‡æ ‡è®¡ç®—å¤±è´¥: {str(e)}'}
    
    elif not ground_truth:
        print("âš ï¸ æœªæä¾›çœŸå®æ ‡ç­¾æ–‡ä»¶")
        basic_metrics['performance'] = {'note': 'æœªæä¾›çœŸå®æ ‡ç­¾'}
    elif not SKLEARN_AVAILABLE:
        print("âš ï¸ sklearnä¸å¯ç”¨")
        basic_metrics['performance'] = {'note': 'sklearnä¸å¯ç”¨'}
    
    return basic_metrics

def load_file_list(file_list_path: str) -> list:
    """ä»æ–‡ä»¶åŠ è½½PDFæ–‡ä»¶åˆ—è¡¨"""
    files = []
    
    if file_list_path.endswith('.txt'):
        with open(file_list_path, 'r', encoding='utf-8') as f:
            files = [line.strip() for line in f if line.strip()]
    elif file_list_path.endswith('.json'):
        with open(file_list_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, list):
                files = data
            elif isinstance(data, dict) and 'generated_files' in data:
                files = data['generated_files']
            elif isinstance(data, dict) and 'files' in data:
                files = data['files']
    
    # éªŒè¯æ–‡ä»¶å­˜åœ¨æ€§
    valid_files = []
    for file_path in files:
        if os.path.exists(file_path) and file_path.lower().endswith('.pdf'):
            valid_files.append(file_path)
    
    return valid_files

def load_ground_truth(ground_truth_path: str) -> Optional[Dict]:
    """åŠ è½½çœŸå®æ ‡ç­¾"""
    if not os.path.exists(ground_truth_path):
        return None
    
    try:
        with open(ground_truth_path, 'r', encoding='utf-8') as f:
            if ground_truth_path.endswith('.json'):
                return json.load(f)
            elif ground_truth_path.endswith('.csv'):
                df = pd.read_csv(ground_truth_path)
                if 'file' in df.columns and 'label' in df.columns:
                    return dict(zip(df['file'], df['label']))
                elif 'filename' in df.columns and 'is_malicious' in df.columns:
                    return dict(zip(df['filename'], df['is_malicious']))
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•åŠ è½½çœŸå®æ ‡ç­¾æ–‡ä»¶ {ground_truth_path}: {e}")
    
    return None

def generate_detailed_report(metrics: Dict, output_path: str):
    """ç”Ÿæˆè¯¦ç»†çš„æ£€æµ‹æŠ¥å‘Š"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    report = f"""
PDFæ¶æ„æ£€æµ‹è¯¦ç»†æŠ¥å‘Š
==========================================
ç”Ÿæˆæ—¶é—´: {timestamp}
æ£€æµ‹é˜ˆå€¼: {metrics.get('threshold_used', 'N/A')}

ğŸ“Š åŸºç¡€ç»Ÿè®¡
-----------
æ€»æ–‡ä»¶æ•°: {metrics['total_files']}
æ¶æ„æ–‡ä»¶æ•°: {metrics['malicious_files']} ({metrics['malicious_rate']:.1%})
æ­£å¸¸æ–‡ä»¶æ•°: {metrics['benign_files']} ({(1-metrics['malicious_rate']):.1%})

ğŸ“ˆ é£é™©åˆ†æ•°åˆ†æ
--------------
å¹³å‡é£é™©åˆ†æ•°: {metrics['avg_risk_score']:.4f}
æ ‡å‡†å·®: {metrics['std_risk_score']:.4f}
æœ€å°å€¼: {metrics['min_risk_score']:.4f}
æœ€å¤§å€¼: {metrics['max_risk_score']:.4f}
ä¸­ä½æ•°: {metrics['median_risk_score']:.4f}
å¹³å‡æ£€æµ‹æ•°é‡: {metrics['avg_detection_count']:.1f}

ğŸ“Š é£é™©åˆ†å¸ƒ
-----------
æä½é£é™© (0-0.1): {metrics['risk_distribution']['very_low']} ä¸ª
ä½é£é™© (0.1-0.3): {metrics['risk_distribution']['low']} ä¸ª
ä¸­ç­‰é£é™© (0.3-0.5): {metrics['risk_distribution']['medium']} ä¸ª
é«˜é£é™© (0.5-0.7): {metrics['risk_distribution']['high']} ä¸ª
æé«˜é£é™© (â‰¥0.7): {metrics['risk_distribution']['very_high']} ä¸ª

ğŸ” æ£€æµ‹ç»Ÿè®¡
-----------
æ€»æ£€æµ‹æ¬¡æ•°: {metrics['detection_statistics']['total_detections']}
æ£€æµ‹ç±»å‹æ•°é‡: {metrics['detection_statistics']['unique_detection_types']}

æ£€æµ‹ç±»å‹åˆ†å¸ƒ:
"""
    
    # æ£€æµ‹ç±»å‹ç»Ÿè®¡
    for det_type, count in sorted(metrics['detection_statistics']['type_distribution'].items(), 
                                key=lambda x: x[1], reverse=True):
        percentage = count / metrics['detection_statistics']['total_detections'] * 100 if metrics['detection_statistics']['total_detections'] > 0 else 0
        report += f"  {det_type}: {count} ({percentage:.1f}%)\n"
    
    # æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'performance' in metrics and isinstance(metrics['performance'], dict) and 'accuracy' in metrics['performance']:
        perf = metrics['performance']
        report += f"""
ğŸ¯ æ€§èƒ½æŒ‡æ ‡ (ä¸çœŸå®æ ‡ç­¾å¯¹æ¯”)
-------------------------
å‡†ç¡®ç‡ (Accuracy): {perf['accuracy']:.4f} ({perf['accuracy']*100:.1f}%)
ç²¾ç¡®ç‡ (Precision): {perf['precision']:.4f} ({perf['precision']*100:.1f}%)
å¬å›ç‡ (Recall): {perf['recall']:.4f} ({perf['recall']*100:.1f}%)
F1åˆ†æ•°: {perf['f1_score']:.4f} ({perf['f1_score']*100:.1f}%)
ç‰¹å¼‚æ€§ (Specificity): {perf['specificity']:.4f} ({perf['specificity']*100:.1f}%)
å‡æ­£ç‡ (FPR): {perf['false_positive_rate']:.4f} ({perf['false_positive_rate']*100:.1f}%)
å‡è´Ÿç‡ (FNR): {perf['false_negative_rate']:.4f} ({perf['false_negative_rate']*100:.1f}%)
ROC AUC: {perf['roc_auc']:.4f}

ğŸ“Š æ··æ·†çŸ©é˜µ
-----------
çœŸé˜´æ€§ (TN): {metrics['confusion_matrix']['true_negatives']}
å‡é˜³æ€§ (FP): {metrics['confusion_matrix']['false_positives']}
å‡é˜´æ€§ (FN): {metrics['confusion_matrix']['false_negatives']}
çœŸé˜³æ€§ (TP): {metrics['confusion_matrix']['true_positives']}

è¦†ç›–ç‡: {metrics.get('ground_truth_coverage', 0)*100:.1f}% ({metrics.get('ground_truth_matched', 0)}/{metrics['total_files']})
"""
    elif 'performance' in metrics:
        report += f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡: {metrics['performance']}\n"
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    return report

def save_comprehensive_results(results: List[Dict], metrics: Dict, output_dir: Path, timestamp: str):
    """ä¿å­˜ç»¼åˆç»“æœ"""
    
    # 1. ä¿å­˜CSVæ ¼å¼çš„ç®€è¦ç»“æœ
    summary_results = []
    for result in results:
        summary = {
            'file_path': result['file'],
            'file_name': Path(result['file']).name,
            'is_malicious': result['is_malicious'],
            'risk_score': result['risk_score'],
            'detection_count': result['detection_count'],
            'file_size': result.get('file_size', 0),
            'processing_time': result.get('processing_time', 0)
        }
        
        # æ·»åŠ ä¸»è¦æ£€æµ‹ç±»å‹
        detection_types = []
        for detection in result.get('detections', []):
            detection_types.append(detection.get('type', 'unknown'))
        summary['detection_types'] = '; '.join(set(detection_types))
        summary['unique_detection_types'] = len(set(detection_types))
        
        summary_results.append(summary)
    
    summary_file = output_dir / f"detection_summary_{timestamp}.csv"
    pd.DataFrame(summary_results).to_csv(summary_file, index=False, encoding='utf-8')
    
    # 2. ä¿å­˜è¯¦ç»†çš„JSONç»“æœ
    details_file = output_dir / f"detection_details_{timestamp}.json"
    save_results(results, str(details_file))
    
    # 3. ä¿å­˜æŒ‡æ ‡
    metrics_file = output_dir / f"detection_metrics_{timestamp}.json"
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, default=str, ensure_ascii=False)
    
    # 4. ä¿å­˜æ¶æ„æ–‡ä»¶åˆ—è¡¨
    malicious_results = [r for r in results if r['is_malicious']]
    if malicious_results:
        malicious_file = output_dir / f"malicious_files_{timestamp}.json"
        save_results(malicious_results, str(malicious_file))
    
    # 5. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    report_file = output_dir / f"detection_report_{timestamp}.txt"
    generate_detailed_report(metrics, str(report_file))
    
    return {
        'summary_csv': str(summary_file),
        'details_json': str(details_file),
        'metrics_json': str(metrics_file),
        'malicious_json': str(malicious_file) if malicious_results else None,
        'report_txt': str(report_file)
    }

def main():
    parser = argparse.ArgumentParser(description='è¿è¡Œæç¤ºè¯æ³¨å…¥æ£€æµ‹ (å¢å¼ºç‰ˆ)')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--input-dir', type=str,
                       help='è¾“å…¥PDFç›®å½•')
    parser.add_argument('--file-list', type=str,
                       help='PDFæ–‡ä»¶åˆ—è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆ.txtæˆ–.jsonï¼‰')
    parser.add_argument('--single-file', type=str,
                       help='å•ä¸ªPDFæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output-dir', type=str,
                       help='è¾“å‡ºç›®å½•ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--detector-type', type=str, default='standard',
                       choices=['standard', 'ensemble'],
                       help='æ£€æµ‹å™¨ç±»å‹')
    parser.add_argument('--threshold', type=float,
                       help='é£é™©åˆ†æ•°é˜ˆå€¼ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--batch-size', type=int, default=20,
                       help='æ‰¹å¤„ç†å¤§å°')
    parser.add_argument('--ground-truth', type=str,
                       help='çœŸå®æ ‡ç­¾æ–‡ä»¶è·¯å¾„ï¼ˆ.jsonæˆ–.csvï¼‰')
    parser.add_argument('--save-details', action='store_true',
                       help='ä¿å­˜è¯¦ç»†æ£€æµ‹ç»“æœ')
    parser.add_argument('--export-csv', action='store_true', default=True,
                       help='å¯¼å‡ºCSVæ ¼å¼ç»“æœ')
    parser.add_argument('--generate-report', action='store_true', default=True,
                       help='ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š')
    parser.add_argument('--save-suspicious-only', action='store_true',
                       help='åªä¿å­˜å¯ç–‘æ–‡ä»¶çš„è¯¦ç»†ç»“æœ')
    parser.add_argument('--max-files', type=int,
                       help='æœ€å¤§å¤„ç†æ–‡ä»¶æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--log-file', type=str,
                       help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--verbose', '-v', action='store_true',
                       help='è¯¦ç»†è¾“å‡º')
    parser.add_argument('--quiet', '-q', action='store_true',
                       help='é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºå…³é”®ä¿¡æ¯')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    if args.quiet:
        log_level = "WARNING"
    elif args.verbose:
        log_level = "DEBUG"
    else:
        log_level = "INFO"
    
    logger = setup_logging(log_level, args.log_file)
    
    if not args.quiet:
        logger.info("=" * 80)
        logger.info("ğŸ” æç¤ºè¯æ³¨å…¥æ£€æµ‹å™¨ (å¢å¼ºç‰ˆ) å¯åŠ¨")
        logger.info("=" * 80)
    
    start_time = time.time()
    
    try:
        # åŠ è½½é…ç½®
        config = load_config(args.config)
        
        # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
        if args.output_dir:
            config['experiment']['output_dir'] = args.output_dir
        
        original_threshold = config['detection']['thresholds']['risk_score']
        if args.threshold:
            config['detection']['thresholds']['risk_score'] = args.threshold
            logger.info(f"é˜ˆå€¼è¦†ç›–: {original_threshold} -> {args.threshold}")
        
        # åŠ è½½çœŸå®æ ‡ç­¾
        ground_truth = None
        if args.ground_truth:
            ground_truth = load_ground_truth(args.ground_truth)
            if ground_truth:
                logger.info(f"åŠ è½½çœŸå®æ ‡ç­¾: {len(ground_truth)} ä¸ªæ–‡ä»¶")
            else:
                logger.warning(f"æ— æ³•åŠ è½½çœŸå®æ ‡ç­¾æ–‡ä»¶: {args.ground_truth}")
        
        # è·å–è¾“å…¥æ–‡ä»¶åˆ—è¡¨
        pdf_files = []
        
        if args.single_file:
            if os.path.exists(args.single_file):
                pdf_files = [args.single_file]
                logger.info(f"æ£€æµ‹å•ä¸ªæ–‡ä»¶: {args.single_file}")
            else:
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {args.single_file}")
                return 1
                
        elif args.file_list:
            logger.info(f"ä»æ–‡ä»¶åˆ—è¡¨åŠ è½½PDF: {args.file_list}")
            pdf_files = load_file_list(args.file_list)
            
        elif args.input_dir:
            logger.info(f"ä»ç›®å½•æ‰«æPDF: {args.input_dir}")
            input_path = Path(args.input_dir)
            pdf_files = [str(f) for f in input_path.rglob("*.pdf")]
            
        else:
            logger.error("âŒ å¿…é¡»æŒ‡å®šè¾“å…¥æºï¼š--single-fileã€--file-list æˆ– --input-dir")
            return 1
        
        if not pdf_files:
            logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶ï¼")
            return 1
        
        # é™åˆ¶æ–‡ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if args.max_files and len(pdf_files) > args.max_files:
            pdf_files = pdf_files[:args.max_files]
            logger.info(f"é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡: {args.max_files}")
        
        logger.info(f"ğŸ“„ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        # åˆ›å»ºæ£€æµ‹å™¨
        if args.detector_type == 'ensemble':
            detector = EnsembleDetector(config)
            logger.info("ğŸ”— ä½¿ç”¨é›†æˆæ£€æµ‹å™¨")
        else:
            detector = PromptInjectionDetector(config)
            logger.info("ğŸ” ä½¿ç”¨æ ‡å‡†æ£€æµ‹å™¨")
        
        # åˆ†æ‰¹å¤„ç†
        batch_size = args.batch_size
        total_batches = (len(pdf_files) + batch_size - 1) // batch_size
        
        all_results = []
        progress = ProgressTracker(len(pdf_files), "æ£€æµ‹PDFæ–‡ä»¶")
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(pdf_files))
            batch_files = pdf_files[start_idx:end_idx]
            
            if not args.quiet:
                logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} "
                           f"({len(batch_files)} ä¸ªæ–‡ä»¶)")
            
            for file_path in batch_files:
                try:
                    file_start_time = time.time()
                    result = detector.detect_injection(file_path)
                    processing_time = time.time() - file_start_time
                    
                    # æ·»åŠ å¤„ç†æ—¶é—´å’Œæ–‡ä»¶å¤§å°
                    result['processing_time'] = processing_time
                    try:
                        result['file_size'] = os.path.getsize(file_path)
                    except:
                        result['file_size'] = 0
                    
                    all_results.append(result)
                    
                    # å®æ—¶æ˜¾ç¤ºç»“æœ
                    risk_score = result['risk_score']
                    is_malicious = result['is_malicious']
                    detection_count = result['detection_count']
                    
                    if not args.quiet:
                        status = "ğŸš¨ æ¶æ„" if is_malicious else "âœ… æ­£å¸¸"
                        file_name = Path(file_path).name
                        if args.verbose:
                            logger.info(f"{status} | {file_name} | "
                                       f"é£é™©: {risk_score:.3f} | æ£€æµ‹: {detection_count} | "
                                       f"æ—¶é—´: {processing_time:.2f}s")
                        elif is_malicious:  # éè¯¦ç»†æ¨¡å¼ä¸‹åªæ˜¾ç¤ºæ¶æ„æ–‡ä»¶
                            logger.warning(f"{status} | {file_name} | é£é™©: {risk_score:.3f}")
                    
                    progress.update()
                    
                except Exception as e:
                    logger.error(f"âŒ æ£€æµ‹å¤±è´¥ {Path(file_path).name}: {e}")
                    progress.update()
                    continue
        
        progress.finish()
        
        total_time = time.time() - start_time
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        logger.info("ğŸ“Š è®¡ç®—æ€§èƒ½æŒ‡æ ‡...")
        metrics = calculate_performance_metrics(
            all_results, 
            ground_truth, 
            config['detection']['thresholds']['risk_score']
        )
        
        # æ·»åŠ å¤„ç†ç»Ÿè®¡
        metrics['processing_statistics'] = {
            'total_processing_time': total_time,
            'avg_processing_time_per_file': total_time / len(all_results) if all_results else 0,
            'files_per_second': len(all_results) / total_time if total_time > 0 else 0
        }
        
        # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
        logger.info("=" * 80)
        logger.info("ğŸ“Š æ£€æµ‹å®Œæˆ - ç»Ÿè®¡ç»“æœ")
        logger.info("=" * 80)
        logger.info(f"æ€»æ–‡ä»¶æ•°: {metrics['total_files']}")
        logger.info(f"æ£€æµ‹ä¸ºæ¶æ„: {metrics['malicious_files']} ({metrics['malicious_rate']*100:.1f}%)")
        logger.info(f"å¹³å‡é£é™©åˆ†æ•°: {metrics['avg_risk_score']:.4f} Â± {metrics['std_risk_score']:.4f}")
        logger.info(f"å¹³å‡æ£€æµ‹æ•°é‡: {metrics['avg_detection_count']:.1f}")
        logger.info(f"æ€»å¤„ç†æ—¶é—´: {total_time:.1f}ç§’ ({metrics['processing_statistics']['files_per_second']:.1f} æ–‡ä»¶/ç§’)")
        
        # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
        if 'performance' in metrics and isinstance(metrics['performance'], dict) and 'accuracy' in metrics['performance']:
            perf = metrics['performance']
            logger.info("=" * 50)
            logger.info("ğŸ¯ æ€§èƒ½æŒ‡æ ‡ (ä¸çœŸå®æ ‡ç­¾å¯¹æ¯”)")
            logger.info("=" * 50)
            logger.info(f"å‡†ç¡®ç‡: {perf['accuracy']:.4f} ({perf['accuracy']*100:.1f}%)")
            logger.info(f"ç²¾ç¡®ç‡: {perf['precision']:.4f} ({perf['precision']*100:.1f}%)")
            logger.info(f"å¬å›ç‡: {perf['recall']:.4f} ({perf['recall']*100:.1f}%)")
            logger.info(f"F1åˆ†æ•°: {perf['f1_score']:.4f} ({perf['f1_score']*100:.1f}%)")
            logger.info(f"ROC AUC: {perf['roc_auc']:.4f}")
        
        # ä¿å­˜ç»“æœ
        output_dir = Path(config['experiment']['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ç»¼åˆç»“æœ
        output_files = save_comprehensive_results(all_results, metrics, output_dir, timestamp)
        
        logger.info("=" * 50)
        logger.info("ğŸ’¾ ç»“æœä¿å­˜å®Œæˆ")
        logger.info("=" * 50)
        for file_type, file_path in output_files.items():
            if file_path:
                logger.info(f"{file_type}: {file_path}")
        
        # æ˜¾ç¤ºæ¶æ„æ–‡ä»¶æŠ¥å‘Š
        malicious_results = [r for r in all_results if r['is_malicious']]
        if malicious_results and not args.quiet:
            logger.info("=" * 50)
            logger.info("ğŸš¨ æ£€æµ‹åˆ°çš„æ¶æ„æ–‡ä»¶")
            logger.info("=" * 50)
            
            for i, result in enumerate(malicious_results[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                file_name = Path(result['file']).name
                risk_score = result['risk_score']
                detection_types = set()
                
                for detection in result.get('detections', []):
                    detection_types.add(detection.get('type', 'unknown'))
                
                logger.info(f"{i:2d}. {file_name}")
                logger.info(f"    é£é™©åˆ†æ•°: {risk_score:.4f}")
                logger.info(f"    æ£€æµ‹ç±»å‹: {', '.join(sorted(detection_types))}")
                logger.info("")
            
            if len(malicious_results) > 10:
                logger.info(f"... è¿˜æœ‰ {len(malicious_results) - 10} ä¸ªæ¶æ„æ–‡ä»¶")
        
        # æ£€æµ‹ç±»å‹ç»Ÿè®¡
        if metrics['detection_statistics']['type_distribution'] and not args.quiet:
            logger.info("=" * 50)
            logger.info("ğŸ“ˆ æ£€æµ‹ç±»å‹ç»Ÿè®¡")
            logger.info("=" * 50)
            for det_type, count in sorted(metrics['detection_statistics']['type_distribution'].items(), 
                                        key=lambda x: x[1], reverse=True):
                percentage = count / metrics['detection_statistics']['total_detections'] * 100
                logger.info(f"  {det_type}: {count} ({percentage:.1f}%)")
        
        logger.info("=" * 80)
        logger.info("ğŸ‰ æ£€æµ‹ä»»åŠ¡å®Œæˆ")
        logger.info("=" * 80)
        
        return 0
        
    except KeyboardInterrupt:
        logger.warning("âš ï¸  ç”¨æˆ·ä¸­æ–­æ£€æµ‹")
        return 1
    except Exception as e:
        logger.error(f"âŒ æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
        return 1

if __name__ == "__main__":
    exit(main())
