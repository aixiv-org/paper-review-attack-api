import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Any, Optional
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    precision_recall_curve, roc_curve, auc
)
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.patches import Rectangle
import json
from pathlib import Path
from .utils import setup_logging, ensure_dir, save_results

logger = setup_logging()

class ExperimentEvaluator:
    """å®éªŒè¯„ä¼°å™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.experiment_config = config['experiment']
        self.output_dir = ensure_dir(self.experiment_config['output_dir'])
        self.results_history = []
        
        # è®¾ç½®matplotlibä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        logger.info(f"å®éªŒè¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    def evaluate_detection_performance(self, 
                                     clean_files: List[str], 
                                     attack_files: List[str], 
                                     detector,
                                     attack_info: Optional[List[Dict]] = None) -> Tuple[pd.DataFrame, Dict]:
        """è¯„ä¼°æ£€æµ‹æ€§èƒ½"""
        
        logger.info(f"å¼€å§‹æ€§èƒ½è¯„ä¼°: {len(clean_files)} ä¸ªæ­£å¸¸æ–‡ä»¶, {len(attack_files)} ä¸ªæ”»å‡»æ–‡ä»¶")
        
        all_results = []
        
        # æµ‹è¯•æ­£å¸¸æ–‡ä»¶ï¼ˆè´Ÿæ ·æœ¬ï¼‰
        logger.info("æµ‹è¯•æ­£å¸¸æ–‡ä»¶...")
        for i, file_path in enumerate(clean_files):
            try:
                result = detector.detect_injection(file_path)
                
                file_result = {
                    'file_path': file_path,
                    'file_name': Path(file_path).name,
                    'label': 0,  # æ­£å¸¸æ–‡ä»¶
                    'predicted': 1 if result['is_malicious'] else 0,
                    'risk_score': result['risk_score'],
                    'detection_count': result['detection_count'],
                    'file_type': 'clean',
                    'attack_type': None,
                    'language': None,
                    'detections': result.get('detections', [])  # ä¿å­˜è¯¦ç»†æ£€æµ‹ä¿¡æ¯
                }
                
                # æ·»åŠ è¯¦ç»†æ£€æµ‹ä¿¡æ¯
                detection_types = [d['type'] for d in result.get('detections', [])]
                file_result['detection_types'] = ', '.join(set(detection_types))
                
                all_results.append(file_result)
                
                if (i + 1) % 10 == 0:
                    logger.info(f"å·²å¤„ç†æ­£å¸¸æ–‡ä»¶: {i + 1}/{len(clean_files)}")
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ­£å¸¸æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        # æµ‹è¯•æ”»å‡»æ–‡ä»¶ï¼ˆæ­£æ ·æœ¬ï¼‰
        logger.info("æµ‹è¯•æ”»å‡»æ–‡ä»¶...")
        attack_info_dict = {}
        if attack_info:
            attack_info_dict = {info['attack_file']: info for info in attack_info}
        
        for i, file_path in enumerate(attack_files):
            try:
                result = detector.detect_injection(file_path)
                
                # è·å–æ”»å‡»ä¿¡æ¯
                attack_details = attack_info_dict.get(file_path, {})
                
                file_result = {
                    'file_path': file_path,
                    'file_name': Path(file_path).name,
                    'label': 1,  # æ”»å‡»æ–‡ä»¶
                    'predicted': 1 if result['is_malicious'] else 0,
                    'risk_score': result['risk_score'],
                    'detection_count': result['detection_count'],
                    'file_type': 'attack',
                    'attack_type': attack_details.get('attack_type', self._extract_attack_type_from_filename(file_path)),
                    'language': attack_details.get('language', self._extract_language_from_filename(file_path)),
                    'detections': result.get('detections', [])
                }
                
                # æ·»åŠ è¯¦ç»†æ£€æµ‹ä¿¡æ¯
                detection_types = [d['type'] for d in result.get('detections', [])]
                file_result['detection_types'] = ', '.join(set(detection_types))
                
                all_results.append(file_result)
                
                if (i + 1) % 10 == 0:
                    logger.info(f"å·²å¤„ç†æ”»å‡»æ–‡ä»¶: {i + 1}/{len(attack_files)}")
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ”»å‡»æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        # åˆ›å»ºç»“æœDataFrame
        df_results = pd.DataFrame(all_results)
        
        if df_results.empty:
            logger.error("æ²¡æœ‰æœ‰æ•ˆçš„æ£€æµ‹ç»“æœ")
            return df_results, {}
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = self._calculate_metrics(df_results)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        self._save_detailed_results(df_results, metrics)
        
        logger.info("æ€§èƒ½è¯„ä¼°å®Œæˆ")
        return df_results, metrics
    
    def _extract_attack_type_from_filename(self, file_path: str) -> str:
        """ä»æ–‡ä»¶åæå–æ”»å‡»ç±»å‹"""
        filename = Path(file_path).name.lower()
        if 'white_text' in filename:
            return 'white_text'
        elif 'metadata' in filename:
            return 'metadata'
        elif 'invisible' in filename:
            return 'invisible_chars'
        elif 'mixed' in filename:
            return 'mixed_language'
        elif 'steganographic' in filename:
            return 'steganographic'
        else:
            return 'unknown'
    
    def _extract_language_from_filename(self, file_path: str) -> str:
        """ä»æ–‡ä»¶åæå–è¯­è¨€"""
        filename = Path(file_path).name.lower()
        if 'english' in filename:
            return 'english'
        elif 'chinese' in filename:
            return 'chinese'
        elif 'japanese' in filename:
            return 'japanese'
        elif 'mixed' in filename:
            return 'mixed'
        else:
            return 'unknown'
    
    def _calculate_metrics(self, df_results: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        y_true = df_results['label'].values
        y_pred = df_results['predicted'].values
        y_scores = df_results['risk_score'].values
        
        # åŸºç¡€æŒ‡æ ‡
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1_score': f1_score(y_true, y_pred, zero_division=0),
            'support': {
                'clean': int(np.sum(y_true == 0)),
                'attack': int(np.sum(y_true == 1))
            }
        }
        
        # ROC AUCï¼ˆå¦‚æœæœ‰æ¦‚ç‡åˆ†æ•°ï¼‰
        if len(np.unique(y_scores)) > 1:
            try:
                metrics['roc_auc'] = roc_auc_score(y_true, y_scores)
            except Exception as e:
                logger.warning(f"ROC AUCè®¡ç®—å¤±è´¥: {e}")
                metrics['roc_auc'] = 0.0
        else:
            metrics['roc_auc'] = 0.0
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(y_true, y_pred)
        metrics['confusion_matrix'] = cm.tolist()
        
        if cm.shape == (2, 2):
            tn, fp, fn, tp = cm.ravel()
            metrics['confusion_matrix_details'] = {
                'true_negative': int(tn),
                'false_positive': int(fp),
                'false_negative': int(fn),
                'true_positive': int(tp)
            }
            
            # è®¡ç®—è¯¯æŠ¥ç‡å’Œæ¼æŠ¥ç‡
            metrics['false_positive_rate'] = fp / (fp + tn) if (fp + tn) > 0 else 0
            metrics['false_negative_rate'] = fn / (fn + tp) if (fn + tp) > 0 else 0
        
        # æŒ‰æ”»å‡»ç±»å‹çš„æ€§èƒ½
        if 'attack_type' in df_results.columns:
            attack_performance = {}
            for attack_type in df_results['attack_type'].dropna().unique():
                if attack_type and attack_type != 'unknown':
                    mask = df_results['attack_type'] == attack_type
                    if mask.sum() > 0:
                        attack_data = df_results[mask]
                        attack_performance[attack_type] = {
                            'count': len(attack_data),
                            'detection_rate': attack_data['predicted'].mean(),
                            'avg_risk_score': attack_data['risk_score'].mean()
                        }
            metrics['performance_by_attack_type'] = attack_performance
        
        # æŒ‰è¯­è¨€çš„æ€§èƒ½
        if 'language' in df_results.columns:
            language_performance = {}
            for language in df_results['language'].dropna().unique():
                if language and language != 'unknown':
                    mask = df_results['language'] == language
                    if mask.sum() > 0:
                        lang_data = df_results[mask]
                        language_performance[language] = {
                            'count': len(lang_data),
                            'detection_rate': lang_data['predicted'].mean(),
                            'avg_risk_score': lang_data['risk_score'].mean()
                        }
            metrics['performance_by_language'] = language_performance
        
        return metrics
    
    def _save_detailed_results(self, df_results: pd.DataFrame, metrics: Dict):
        """ä¿å­˜è¯¦ç»†ç»“æœ"""
        timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜CSVç»“æœ
        csv_file = Path(self.output_dir) / f"detection_results_{timestamp}.csv"
        df_results.to_csv(csv_file, index=False, encoding='utf-8')
        
        # ä¿å­˜JSONæŒ‡æ ‡
        json_file = Path(self.output_dir) / f"metrics_{timestamp}.json"
        save_results(metrics, str(json_file))
        
        logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜: {csv_file}, {json_file}")
    
    def plot_performance_analysis(self, df_results: pd.DataFrame, 
                                metrics: Dict, save_plots: bool = True) -> Dict[str, Any]:
        """ç»˜åˆ¶æ€§èƒ½åˆ†æå›¾è¡¨ - åˆ†ç¦»ç‰ˆæœ¬"""
        
        config = self.config['experiment']['visualization']
        figsize = tuple(config.get('figsize', [12, 8]))
        dpi = config.get('dpi', 300)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if save_plots:
            timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
            plots_dir = Path(self.output_dir) / f"plots_{timestamp}"
            plots_dir.mkdir(parents=True, exist_ok=True)
        
        plot_files = {}
        
        # 1. æ··æ·†çŸ©é˜µ
        logger.info("ç”Ÿæˆæ··æ·†çŸ©é˜µ...")
        fig1, ax1 = plt.subplots(figsize=(8, 6))
        cm = np.array(metrics['confusion_matrix'])
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
                   xticklabels=['Normal', 'Attack'],
                   yticklabels=['Normal', 'Attack'],
                   cbar_kws={'label': 'Count'},
                   annot_kws={'size': 16, 'weight': 'bold'})
        
        ax1.set_title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)
        ax1.set_xlabel('Predicted Label', fontsize=12)
        ax1.set_ylabel('True Label', fontsize=12)
        
        # æ·»åŠ å‡†ç¡®ç‡ä¿¡æ¯
        accuracy = metrics['accuracy']
        ax1.text(0.5, -0.15, f'Accuracy: {accuracy:.3f}', 
                 ha='center', transform=ax1.transAxes, fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        if save_plots:
            plot_file = plots_dir / "01_confusion_matrix.png"
            plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
            plot_files['confusion_matrix'] = plot_file
            logger.info(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜: {plot_file}")
        plt.show()
        plt.close()
        
        # 2. é£é™©åˆ†æ•°åˆ†å¸ƒ
        logger.info("ç”Ÿæˆé£é™©åˆ†æ•°åˆ†å¸ƒå›¾...")
        fig2, ax2 = plt.subplots(figsize=(12, 6))
        clean_scores = df_results[df_results['label']==0]['risk_score']
        attack_scores = df_results[df_results['label']==1]['risk_score']
        
        # ç»˜åˆ¶ç›´æ–¹å›¾
        bins = np.linspace(0, 1, 21)
        alpha = 0.7
        
        if len(clean_scores) > 0:
            ax2.hist(clean_scores, alpha=alpha, label=f'Normal Files (n={len(clean_scores)})', 
                     bins=bins, density=True, color='skyblue', edgecolor='navy', linewidth=1)
        
        if len(attack_scores) > 0:
            ax2.hist(attack_scores, alpha=alpha, label=f'Attack Files (n={len(attack_scores)})', 
                     bins=bins, density=True, color='lightcoral', edgecolor='darkred', linewidth=1)
        
        # æ·»åŠ é˜ˆå€¼çº¿
        threshold = self.config['detection']['thresholds']['risk_score']
        ax2.axvline(x=threshold, color='red', linestyle='--', linewidth=3, 
                   label=f'Threshold ({threshold})')
        
        ax2.set_title('Risk Score Distribution', fontsize=16, fontweight='bold')
        ax2.set_xlabel('Risk Score', fontsize=12)
        ax2.set_ylabel('Density', fontsize=12)
        ax2.legend(fontsize=11, loc='upper left')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if len(clean_scores) > 0 and len(attack_scores) > 0:
            stats_text = f"""Normal Files:
Mean: {clean_scores.mean():.3f}
Std: {clean_scores.std():.3f}

Attack Files:
Mean: {attack_scores.mean():.3f}
Std: {attack_scores.std():.3f}"""
            
            ax2.text(0.02, 0.98, stats_text, transform=ax2.transAxes, 
                     verticalalignment='top', fontsize=10,
                     bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        plt.tight_layout()
        if save_plots:
            plot_file = plots_dir / "02_risk_score_distribution.png"
            plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
            plot_files['risk_distribution'] = plot_file
            logger.info(f"é£é™©åˆ†æ•°åˆ†å¸ƒå›¾å·²ä¿å­˜: {plot_file}")
        plt.show()
        plt.close()
        
        # 3. æ€§èƒ½æŒ‡æ ‡æ¡å½¢å›¾
        logger.info("ç”Ÿæˆæ€§èƒ½æŒ‡æ ‡å›¾...")
        fig3, ax3 = plt.subplots(figsize=(12, 6))
        metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
        metric_values = [
            metrics['accuracy'], metrics['precision'], 
            metrics['recall'], metrics['f1_score'], metrics['roc_auc']
        ]
        
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
        bars = ax3.bar(metric_names, metric_values, color=colors, alpha=0.8, 
                      edgecolor='black', linewidth=1.5)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, value in zip(bars, metric_values):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=12)
        
        ax3.set_title('Performance Metrics', fontsize=16, fontweight='bold')
        ax3.set_ylabel('Score', fontsize=12)
        ax3.set_ylim(0, 1.1)
        ax3.grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ åŸºå‡†çº¿
        ax3.axhline(y=0.8, color='red', linestyle=':', alpha=0.7, linewidth=2, 
                   label='Good Baseline (0.8)')
        ax3.legend(loc='upper right')
        
        plt.xticks(rotation=0)
        plt.tight_layout()
        if save_plots:
            plot_file = plots_dir / "03_performance_metrics.png"
            plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
            plot_files['performance_metrics'] = plot_file
            logger.info(f"æ€§èƒ½æŒ‡æ ‡å›¾å·²ä¿å­˜: {plot_file}")
        plt.show()
        plt.close()
        
        # 4. ROCæ›²çº¿å’ŒPRæ›²çº¿
        logger.info("ç”ŸæˆROCå’ŒPRæ›²çº¿...")
        fig4, (ax4_1, ax4_2) = plt.subplots(1, 2, figsize=(15, 6))
        
        try:
            # ROCæ›²çº¿
            fpr, tpr, _ = roc_curve(df_results['label'], df_results['risk_score'])
            roc_auc = auc(fpr, tpr)
            
            ax4_1.plot(fpr, tpr, color='darkorange', lw=3, 
                      label=f'ROC Curve (AUC = {roc_auc:.3f})')
            ax4_1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', 
                      label='Random Classifier')
            ax4_1.set_xlim([0.0, 1.0])
            ax4_1.set_ylim([0.0, 1.05])
            ax4_1.set_xlabel('False Positive Rate', fontsize=12)
            ax4_1.set_ylabel('True Positive Rate', fontsize=12)
            ax4_1.set_title('ROC Curve', fontsize=14, fontweight='bold')
            ax4_1.legend(loc="lower right")
            ax4_1.grid(True, alpha=0.3)
            
            # PRæ›²çº¿
            precision_curve, recall_curve, _ = precision_recall_curve(
                df_results['label'], df_results['risk_score'])
            pr_auc = auc(recall_curve, precision_curve)
            
            ax4_2.plot(recall_curve, precision_curve, color='blue', lw=3,
                      label=f'PR Curve (AUC = {pr_auc:.3f})')
            ax4_2.set_xlabel('Recall', fontsize=12)
            ax4_2.set_ylabel('Precision', fontsize=12)
            ax4_2.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')
            ax4_2.set_xlim([0.0, 1.0])
            ax4_2.set_ylim([0.0, 1.05])
            ax4_2.legend()
            ax4_2.grid(True, alpha=0.3)
            
        except Exception as e:
            logger.error(f"ROC/PRæ›²çº¿ç”Ÿæˆå¤±è´¥: {e}")
            ax4_1.text(0.5, 0.5, f'ROC curve error: {str(e)}', 
                      ha='center', va='center', transform=ax4_1.transAxes)
            ax4_2.text(0.5, 0.5, f'PR curve error: {str(e)}', 
                      ha='center', va='center', transform=ax4_2.transAxes)
        
        plt.tight_layout()
        if save_plots:
            plot_file = plots_dir / "04_roc_pr_curves.png"
            plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
            plot_files['roc_pr_curves'] = plot_file
            logger.info(f"ROCå’ŒPRæ›²çº¿å·²ä¿å­˜: {plot_file}")
        plt.show()
        plt.close()
        
        # 5. æŒ‰æ”»å‡»ç±»å‹çš„æ€§èƒ½
        if 'performance_by_attack_type' in metrics and metrics['performance_by_attack_type']:
            logger.info("ç”Ÿæˆæ”»å‡»ç±»å‹æ€§èƒ½å›¾...")
            fig5, ax5 = plt.subplots(figsize=(12, 6))
            attack_types = list(metrics['performance_by_attack_type'].keys())
            detection_rates = [metrics['performance_by_attack_type'][at]['detection_rate'] 
                              for at in attack_types]
            counts = [metrics['performance_by_attack_type'][at]['count'] 
                     for at in attack_types]
            
            bars = ax5.bar(attack_types, detection_rates, color='lightblue', 
                          alpha=0.8, edgecolor='darkblue', linewidth=1.5)
            
            # æ·»åŠ æ ·æœ¬æ•°é‡æ ‡ç­¾
            for bar, rate, count in zip(bars, detection_rates, counts):
                height = bar.get_height()
                ax5.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{rate:.3f}\n(n={count})', ha='center', va='bottom', 
                        fontweight='bold')
            
            ax5.set_title('Detection Rate by Attack Type', fontsize=16, fontweight='bold')
            ax5.set_ylabel('Detection Rate', fontsize=12)
            ax5.set_ylim(0, 1.1)
            ax5.grid(True, alpha=0.3, axis='y')
            plt.xticks(rotation=45)
            plt.tight_layout()
            if save_plots:
                plot_file = plots_dir / "05_attack_type_performance.png"
                plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
                plot_files['attack_type_performance'] = plot_file
                logger.info(f"æ”»å‡»ç±»å‹æ€§èƒ½å›¾å·²ä¿å­˜: {plot_file}")
            plt.show()
            plt.close()
        
        # 6. æŒ‰è¯­è¨€çš„æ€§èƒ½
        if 'performance_by_language' in metrics and metrics['performance_by_language']:
            logger.info("ç”Ÿæˆè¯­è¨€æ€§èƒ½å›¾...")
            fig6, ax6 = plt.subplots(figsize=(10, 6))
            languages = list(metrics['performance_by_language'].keys())
            detection_rates = [metrics['performance_by_language'][lang]['detection_rate'] 
                              for lang in languages]
            avg_scores = [metrics['performance_by_language'][lang]['avg_risk_score'] 
                         for lang in languages]
            counts = [metrics['performance_by_language'][lang]['count'] 
                     for lang in languages]
            
            # åŒyè½´å›¾
            ax6_twin = ax6.twinx()
            
            x_pos = np.arange(len(languages))
            width = 0.35
            
            bars1 = ax6.bar(x_pos - width/2, detection_rates, width, 
                           label='Detection Rate', color='lightblue', alpha=0.8,
                           edgecolor='darkblue')
            bars2 = ax6_twin.bar(x_pos + width/2, avg_scores, width,
                                label='Avg Risk Score', color='lightcoral', alpha=0.8,
                                edgecolor='darkred')
            
            # æ·»åŠ æ ‡ç­¾
            for i, (bar1, bar2, rate, score, count) in enumerate(zip(bars1, bars2, detection_rates, avg_scores, counts)):
                ax6.text(bar1.get_x() + bar1.get_width()/2., bar1.get_height() + 0.01,
                        f'{rate:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
                ax6_twin.text(bar2.get_x() + bar2.get_width()/2., bar2.get_height() + 0.01,
                             f'{score:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')
                ax6.text(i, -0.15, f'n={count}', ha='center', va='top', 
                        transform=ax6.get_xaxis_transform(), fontsize=9)
            
            ax6.set_title('Performance by Language', fontsize=16, fontweight='bold')
            ax6.set_ylabel('Detection Rate', fontsize=12, color='blue')
            ax6_twin.set_ylabel('Average Risk Score', fontsize=12, color='red')
            ax6.set_xticks(x_pos)
            ax6.set_xticklabels(languages)
            ax6.set_ylim(0, 1.1)
            ax6_twin.set_ylim(0, 1.1)
            
            # å›¾ä¾‹
            lines1, labels1 = ax6.get_legend_handles_labels()
            lines2, labels2 = ax6_twin.get_legend_handles_labels()
            ax6.legend(lines1 + lines2, labels1 + labels2, loc='upper left')
            
            ax6.grid(True, alpha=0.3, axis='y')
            plt.tight_layout()
            if save_plots:
                plot_file = plots_dir / "06_language_performance.png"
                plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
                plot_files['language_performance'] = plot_file
                logger.info(f"è¯­è¨€æ€§èƒ½å›¾å·²ä¿å­˜: {plot_file}")
            plt.show()
            plt.close()
        
        # 7. é£é™©åˆ†æ•°æ•£ç‚¹å›¾
        logger.info("ç”Ÿæˆé£é™©åˆ†æ•°æ•£ç‚¹å›¾...")
        fig7, ax7 = plt.subplots(figsize=(14, 6))
        
        # æ­£å¸¸æ–‡ä»¶
        clean_data = df_results[df_results['label']==0]
        if len(clean_data) > 0:
            ax7.scatter(range(len(clean_data)), clean_data['risk_score'], 
                       alpha=0.7, label=f'Normal Files (n={len(clean_data)})', 
                       color='blue', s=40, marker='o', edgecolors='darkblue')
        
        # æ”»å‡»æ–‡ä»¶
        attack_data = df_results[df_results['label']==1]
        if len(attack_data) > 0:
            attack_start = len(clean_data) if len(clean_data) > 0 else 0
            ax7.scatter(range(attack_start, attack_start + len(attack_data)), 
                       attack_data['risk_score'], alpha=0.7, 
                       label=f'Attack Files (n={len(attack_data)})', 
                       color='red', s=40, marker='^', edgecolors='darkred')
        
        # é˜ˆå€¼çº¿
        threshold = self.config['detection']['thresholds']['risk_score']
        ax7.axhline(y=threshold, color='green', linestyle='--', linewidth=3,
                   label=f'Threshold ({threshold})')
        
        ax7.set_title('Risk Score Distribution by File', fontsize=16, fontweight='bold')
        ax7.set_xlabel('File Index', fontsize=12)
        ax7.set_ylabel('Risk Score', fontsize=12)
        ax7.legend(fontsize=11)
        ax7.grid(True, alpha=0.3)
        ax7.set_ylim(-0.05, 1.05)
        
        plt.tight_layout()
        if save_plots:
            plot_file = plots_dir / "07_risk_score_scatter.png"
            plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
            plot_files['risk_score_scatter'] = plot_file
            logger.info(f"é£é™©åˆ†æ•°æ•£ç‚¹å›¾å·²ä¿å­˜: {plot_file}")
        plt.show()
        plt.close()
        
        # 8. æ£€æµ‹ç±»å‹ç»Ÿè®¡
        logger.info("ç”Ÿæˆæ£€æµ‹ç±»å‹ç»Ÿè®¡å›¾...")
        fig8, ax8 = plt.subplots(figsize=(12, 8))
        
        # ç»Ÿè®¡æ£€æµ‹ç±»å‹
        detection_type_counts = {}
        for _, result in df_results.iterrows():
            for detection in result.get('detections', []):
                det_type = detection.get('type', 'unknown')
                detection_type_counts[det_type] = detection_type_counts.get(det_type, 0) + 1
        
        if detection_type_counts:
            # æŒ‰æ•°é‡æ’åº
            sorted_items = sorted(detection_type_counts.items(), key=lambda x: x[1], reverse=True)
            types = [item[0] for item in sorted_items]
            counts = [item[1] for item in sorted_items]
            
            # æ¨ªå‘æ¡å½¢å›¾
            bars = ax8.barh(types, counts, color='lightgreen', alpha=0.8, 
                           edgecolor='darkgreen', linewidth=1.5)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, counts):
                width = bar.get_width()
                ax8.text(width + max(counts) * 0.01, bar.get_y() + bar.get_height()/2.,
                        f'{count}', ha='left', va='center', fontweight='bold', fontsize=11)
            
            ax8.set_title('Detection Type Frequency', fontsize=16, fontweight='bold')
            ax8.set_xlabel('Count', fontsize=12)
            ax8.set_ylabel('Detection Type', fontsize=12)
            ax8.grid(True, alpha=0.3, axis='x')
            
            plt.tight_layout()
            if save_plots:
                plot_file = plots_dir / "08_detection_type_stats.png"
                plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
                plot_files['detection_type_stats'] = plot_file
                logger.info(f"æ£€æµ‹ç±»å‹ç»Ÿè®¡å›¾å·²ä¿å­˜: {plot_file}")
            plt.show()
            plt.close()
        
        # 9. æ£€æµ‹æ•°é‡åˆ†å¸ƒ
        logger.info("ç”Ÿæˆæ£€æµ‹æ•°é‡åˆ†å¸ƒå›¾...")
        fig9, ax9 = plt.subplots(figsize=(10, 6))
        
        clean_detection_counts = df_results[df_results['label']==0]['detection_count']
        attack_detection_counts = df_results[df_results['label']==1]['detection_count']
        
        box_data = []
        labels = []
        if len(clean_detection_counts) > 0:
            box_data.append(clean_detection_counts)
            labels.append(f'Normal\n(n={len(clean_detection_counts)})')
        
        if len(attack_detection_counts) > 0:
            box_data.append(attack_detection_counts)
            labels.append(f'Attack\n(n={len(attack_detection_counts)})')
        
        if box_data:
            bp = ax9.boxplot(box_data, labels=labels, patch_artist=True)
            
            # è®¾ç½®é¢œè‰²
            colors = ['lightblue', 'lightcoral']
            for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):
                patch.set_facecolor(color)
                patch.set_alpha(0.8)
        
        ax9.set_title('Detection Count Distribution', fontsize=16, fontweight='bold')
        ax9.set_ylabel('Number of Detections', fontsize=12)
        ax9.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        if save_plots:
            plot_file = plots_dir / "09_detection_count_distribution.png"
            plt.savefig(plot_file, dpi=dpi, bbox_inches='tight')
            plot_files['detection_count_distribution'] = plot_file
            logger.info(f"æ£€æµ‹æ•°é‡åˆ†å¸ƒå›¾å·²ä¿å­˜: {plot_file}")
        plt.show()
        plt.close()
        
        # ç”Ÿæˆå›¾è¡¨ç´¢å¼•HTMLæ–‡ä»¶
        if save_plots:
            self._generate_plots_index(plots_dir, plot_files, metrics)
            logger.info(f"æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {plots_dir}")
        
        return {'plots_directory': plots_dir if save_plots else None, 'plot_files': plot_files}
    
    def _generate_plots_index(self, plots_dir: Path, plot_files: Dict[str, Path], metrics: Dict):
        """ç”Ÿæˆå›¾è¡¨ç´¢å¼•HTMLæ–‡ä»¶"""
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Detection Performance Analysis Report</title>
    <meta charset="UTF-8">
    <style>
        body {{ 
            font-family: Arial, sans-serif; 
            margin: 20px; 
            background-color: #f5f5f5;
        }}
        .header {{
            background-color: #2c3e50;
            color: white;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
        }}
        .metrics-summary {{
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        .metrics-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-top: 15px;
        }}
        .metric-card {{
            background-color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            text-align: center;
        }}
        .metric-value {{
            font-size: 24px;
            font-weight: bold;
            color: #27ae60;
        }}
        .plot-container {{ 
            margin: 20px 0; 
            padding: 20px; 
            border: 1px solid #ddd; 
            background-color: white;
            border-radius: 10px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }}
        .plot-title {{ 
            font-size: 18px; 
            font-weight: bold; 
            margin-bottom: 10px; 
            color: #2c3e50;
        }}
        img {{ 
            max-width: 100%; 
            height: auto; 
            border-radius: 5px;
        }}
        .timestamp {{
            color: #7f8c8d;
            font-style: italic;
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸ“Š Detection Performance Analysis Report</h1>
        <p class="timestamp">Generated: {pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")}</p>
    </div>
    
    <div class="metrics-summary">
        <h2>ğŸ“ˆ Performance Summary</h2>
        <div class="metrics-grid">
            <div class="metric-card">
                <div class="metric-value">{metrics.get('accuracy', 0):.3f}</div>
                <div>Accuracy</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{metrics.get('precision', 0):.3f}</div>
                <div>Precision</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{metrics.get('recall', 0):.3f}</div>
                <div>Recall</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{metrics.get('f1_score', 0):.3f}</div>
                <div>F1-Score</div>
            </div>
            <div class="metric-card">
                <div class="metric-value">{metrics.get('roc_auc', 0):.3f}</div>
                <div>ROC AUC</div>
            </div>
        </div>
        
        <h3>ğŸ“ Dataset Information</h3>
        <p>Total Files: {metrics.get('support', {}).get('clean', 0) + metrics.get('support', {}).get('attack', 0)}</p>
        <p>Normal Files: {metrics.get('support', {}).get('clean', 0)} | Attack Files: {metrics.get('support', {}).get('attack', 0)}</p>
    </div>
"""
        
        plot_descriptions = {
            'confusion_matrix': 'ğŸ¯ æ··æ·†çŸ©é˜µ - æ˜¾ç¤ºåˆ†ç±»å‡†ç¡®æ€§ï¼ŒçœŸé˜³æ€§ã€å‡é˜³æ€§ã€çœŸé˜´æ€§ã€å‡é˜´æ€§çš„åˆ†å¸ƒ',
            'risk_distribution': 'ğŸ“Š é£é™©åˆ†æ•°åˆ†å¸ƒ - æ­£å¸¸æ–‡ä»¶ vs æ”»å‡»æ–‡ä»¶çš„é£é™©åˆ†æ•°å¯¹æ¯”åˆ†æ',
            'performance_metrics': 'ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ - å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ã€ROC AUCç­‰å…³é”®æŒ‡æ ‡',
            'roc_pr_curves': 'ğŸ“‰ ROCå’ŒPRæ›²çº¿ - åˆ†ç±»å™¨åœ¨ä¸åŒé˜ˆå€¼ä¸‹çš„æ€§èƒ½è¯„ä¼°æ›²çº¿',
            'attack_type_performance': 'ğŸ­ æ”»å‡»ç±»å‹æ€§èƒ½ - æŒ‰ä¸åŒæ”»å‡»ç±»å‹åˆ†æçš„æ£€æµ‹æˆåŠŸç‡',
            'language_performance': 'ğŸŒ è¯­è¨€æ€§èƒ½åˆ†æ - æŒ‰ä¸åŒè¯­è¨€åˆ†æçš„æ£€æµ‹æ•ˆæœå¯¹æ¯”',
            'risk_score_scatter': 'ğŸ” é£é™©åˆ†æ•°æ•£ç‚¹å›¾ - æ¯ä¸ªæ–‡ä»¶çš„é£é™©åˆ†æ•°åˆ†å¸ƒå¯è§†åŒ–',
            'detection_type_stats': 'ğŸ“‹ æ£€æµ‹ç±»å‹ç»Ÿè®¡ - å„ç§æ£€æµ‹æœºåˆ¶çš„è§¦å‘é¢‘ç‡ç»Ÿè®¡',
            'detection_count_distribution': 'ğŸ“¦ æ£€æµ‹æ•°é‡åˆ†å¸ƒ - æ­£å¸¸æ–‡ä»¶ vs æ”»å‡»æ–‡ä»¶çš„æ£€æµ‹æ¬¡æ•°ç®±çº¿å›¾'
        }
        
        for plot_key, plot_file in plot_files.items():
            if plot_file.exists():
                description = plot_descriptions.get(plot_key, plot_key)
                html_content += f"""
            <div class="plot-container">
                <div class="plot-title">{description}</div>
                <img src="{plot_file.name}" alt="{description}">
            </div>
            """
        
        html_content += """
    <div style="text-align: center; margin-top: 40px; color: #7f8c8d;">
        <p>ğŸ“§ Generated by Paper Review Attack Detection System</p>
    </div>
</body>
</html>
"""
        
        html_file = plots_dir / "index.html"
        with open(html_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"ğŸ“‹ å›¾è¡¨ç´¢å¼•å·²ç”Ÿæˆ: {html_file}")
    
    def generate_report(self, df_results: pd.DataFrame, metrics: Dict) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        
        timestamp = pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
# è®ºæ–‡å®¡ç¨¿æ”»å‡»æ£€æµ‹å®éªŒæŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {timestamp}

## 1. å®éªŒæ¦‚è¿°

### æ•°æ®é›†ç»Ÿè®¡
- æ€»æ–‡ä»¶æ•°: {len(df_results)}
- æ­£å¸¸æ–‡ä»¶æ•°: {metrics['support']['clean']}
- æ”»å‡»æ–‡ä»¶æ•°: {metrics['support']['attack']}
- æ”»å‡»æ–‡ä»¶æ¯”ä¾‹: {metrics['support']['attack'] / len(df_results):.1%}

## 2. æ£€æµ‹æ€§èƒ½

### æ€»ä½“æ€§èƒ½æŒ‡æ ‡
- **å‡†ç¡®ç‡ (Accuracy)**: {metrics['accuracy']:.3f}
- **ç²¾ç¡®ç‡ (Precision)**: {metrics['precision']:.3f}
- **å¬å›ç‡ (Recall)**: {metrics['recall']:.3f}
- **F1åˆ†æ•°**: {metrics['f1_score']:.3f}
- **ROC AUC**: {metrics['roc_auc']:.3f}

### æ··æ·†çŸ©é˜µ
"""
        
        if 'confusion_matrix_details' in metrics:
            cm_details = metrics['confusion_matrix_details']
            report += f"""
- çœŸé˜´æ€§ (True Negative): {cm_details['true_negative']}
- å‡é˜³æ€§ (False Positive): {cm_details['false_positive']}
- å‡é˜´æ€§ (False Negative): {cm_details['false_negative']}
- çœŸé˜³æ€§ (True Positive): {cm_details['true_positive']}

### è¯¯æŠ¥åˆ†æ
- è¯¯æŠ¥ç‡ (False Positive Rate): {metrics['false_positive_rate']:.3f}
- æ¼æŠ¥ç‡ (False Negative Rate): {metrics['false_negative_rate']:.3f}
"""
        
        # æŒ‰æ”»å‡»ç±»å‹çš„æ€§èƒ½
        if 'performance_by_attack_type' in metrics:
            report += "\n## 3. æŒ‰æ”»å‡»ç±»å‹çš„æ£€æµ‹æ€§èƒ½\n\n"
            for attack_type, perf in metrics['performance_by_attack_type'].items():
                report += f"### {attack_type}\n"
                report += f"- æ ·æœ¬æ•°é‡: {perf['count']}\n"
                report += f"- æ£€æµ‹ç‡: {perf['detection_rate']:.3f}\n"
                report += f"- å¹³å‡é£é™©åˆ†æ•°: {perf['avg_risk_score']:.3f}\n\n"
        
        # æŒ‰è¯­è¨€çš„æ€§èƒ½
        if 'performance_by_language' in metrics:
            report += "\n## 4. æŒ‰è¯­è¨€çš„æ£€æµ‹æ€§èƒ½\n\n"
            for language, perf in metrics['performance_by_language'].items():
                report += f"### {language}\n"
                report += f"- æ ·æœ¬æ•°é‡: {perf['count']}\n"
                report += f"- æ£€æµ‹ç‡: {perf['detection_rate']:.3f}\n"
                report += f"- å¹³å‡é£é™©åˆ†æ•°: {perf['avg_risk_score']:.3f}\n\n"
        
        # é£é™©åˆ†æ•°åˆ†æ
        clean_scores = df_results[df_results['label']==0]['risk_score']
        attack_scores = df_results[df_results['label']==1]['risk_score']
        
        report += f"""
## 5. é£é™©åˆ†æ•°åˆ†æ

### æ­£å¸¸æ–‡ä»¶é£é™©åˆ†æ•°
- å¹³å‡å€¼: {clean_scores.mean():.3f}
- æ ‡å‡†å·®: {clean_scores.std():.3f}
- æœ€å¤§å€¼: {clean_scores.max():.3f}
- è¶…è¿‡é˜ˆå€¼çš„æ¯”ä¾‹: {(clean_scores > self.config['detection']['thresholds']['risk_score']).mean():.3f}

### æ”»å‡»æ–‡ä»¶é£é™©åˆ†æ•°
- å¹³å‡å€¼: {attack_scores.mean():.3f}
- æ ‡å‡†å·®: {attack_scores.std():.3f}
- æœ€å°å€¼: {attack_scores.min():.3f}
- è¶…è¿‡é˜ˆå€¼çš„æ¯”ä¾‹: {(attack_scores > self.config['detection']['thresholds']['risk_score']).mean():.3f}

## 6. å»ºè®®å’Œæ”¹è¿›æ–¹å‘

### åŸºäºå®éªŒç»“æœçš„å»ºè®®:
"""
        
        # æ ¹æ®ç»“æœæä¾›å»ºè®®
        if metrics['false_positive_rate'] > 0.1:
            report += "- **é«˜è¯¯æŠ¥ç‡**: å»ºè®®è°ƒæ•´æ£€æµ‹é˜ˆå€¼æˆ–ä¼˜åŒ–å…³é”®è¯åº“ä»¥å‡å°‘è¯¯æŠ¥\n"
        
        if metrics['false_negative_rate'] > 0.1:
            report += "- **é«˜æ¼æŠ¥ç‡**: å»ºè®®å¢å¼ºæ£€æµ‹ç®—æ³•æˆ–æ·»åŠ æ–°çš„æ£€æµ‹ç»´åº¦\n"
        
        if metrics['roc_auc'] < 0.8:
            report += "- **ROC AUCè¾ƒä½**: å»ºè®®æ”¹è¿›é£é™©åˆ†æ•°è®¡ç®—æ–¹æ³•\n"
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp_file = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
        report_file = Path(self.output_dir) / f"experiment_report_{timestamp_file}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"å®éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report
    
    def compare_experiments(self, experiment_results: List[Dict]) -> Dict:
        """æ¯”è¾ƒå¤šä¸ªå®éªŒç»“æœ"""
        if len(experiment_results) < 2:
            logger.warning("éœ€è¦è‡³å°‘2ä¸ªå®éªŒç»“æœè¿›è¡Œæ¯”è¾ƒ")
            return {}
        
        comparison = {
            'experiment_count': len(experiment_results),
            'metrics_comparison': {},
            'best_experiment': None,
            'improvement_suggestions': []
        }
        
        # æ¯”è¾ƒæŒ‡æ ‡
        metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']
        
        for metric in metrics_to_compare:
            values = [exp.get(metric, 0) for exp in experiment_results]
            comparison['metrics_comparison'][metric] = {
                'values': values,
                'best_index': np.argmax(values),
                'best_value': max(values),
                'worst_value': min(values),
                'improvement': max(values) - min(values)
            }
        
        # ç¡®å®šæœ€ä½³å®éªŒï¼ˆåŸºäºF1åˆ†æ•°ï¼‰
        f1_scores = [exp.get('f1_score', 0) for exp in experiment_results]
        best_idx = np.argmax(f1_scores)
        comparison['best_experiment'] = {
            'index': best_idx,
            'f1_score': f1_scores[best_idx]
        }
        
        return comparison
