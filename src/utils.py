import os
import yaml
import logging
import json
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, Tuple
from loguru import logger
import hashlib
import re
import warnings
import fitz
import psutil
import time
import threading
import queue
import pickle
import gzip
import sqlite3
from datetime import datetime, timedelta
from functools import wraps, lru_cache
from collections import defaultdict, deque
import numpy as np

# ============================================================================
# ğŸ”§ åŸºç¡€å·¥å…·å‡½æ•° (ä¿æŒä½ çš„åŸæœ‰ä»£ç )
# ============================================================================

def setup_logging(log_level: str = "INFO", log_file: Optional[str] = None, config: Dict = None):
    """è®¾ç½®å¢å¼ºçš„æ—¥å¿—é…ç½®"""
    logger.remove()  # ç§»é™¤é»˜è®¤handler
    
    # å¦‚æœæä¾›äº†é…ç½®ï¼Œä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
    if config:
        log_config = config.get('logging', {})
        log_level = log_config.get('levels', {}).get('console', log_level)
        if not log_file:
            log_file = log_config.get('files', {}).get('main_log')
    
    # æ§åˆ¶å°è¾“å‡º
    console_format = ("<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
                     "<level>{level: <8}</level> | "
                     "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - "
                     "<level>{message}</level>")
    
    logger.add(
        lambda msg: print(msg, end=""),
        format=console_format,
        level=log_level,
        colorize=True
    )
    
    # æ–‡ä»¶è¾“å‡º
    if log_file:
        ensure_dir(os.path.dirname(log_file))
        file_format = "{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}"
        
        logger.add(
            log_file,
            format=file_format,
            level="DEBUG",
            rotation="10 MB",
            retention="30 days",
            compression="zip"
        )
        
        # é”™è¯¯æ—¥å¿—å•ç‹¬æ–‡ä»¶
        if config and config.get('logging', {}).get('files', {}).get('error_log'):
            error_log = config['logging']['files']['error_log']
            ensure_dir(os.path.dirname(error_log))
            logger.add(
                error_log,
                format=file_format,
                level="ERROR",
                rotation="5 MB",
                retention="30 days"
            )
    
    return logger

@lru_cache(maxsize=1)
def load_config(config_path: str = "config/config.yaml") -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # éªŒè¯é…ç½®
        config = validate_and_fill_config(config)
        
        logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except Exception as e:
        logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        raise

def validate_and_fill_config(config: Dict[str, Any]) -> Dict[str, Any]:
    """éªŒè¯å¹¶å¡«å……é…ç½®é»˜è®¤å€¼"""
    # ç¡®ä¿å¿…è¦çš„é…ç½®å­˜åœ¨
    default_config = {
        'detection': {
            'offline_mode': True,
            'thresholds': {
                'risk_score': 0.35,
                'sentiment_confidence': 0.85,
                'keyword_match': 0.7,
                'detection_count': 2
            },
            'detection_weights': {
                'semantic_injection': 1.8,
                'contextual_anomaly': 1.6,
                'keyword_injection': 1.4,
                'small_text_injection': 0.4
            },
            'false_positive_suppression': {
                'enabled': True,
                'max_small_text_ratio': 0.03
            }
        },
        'experiment': {
            'output_dir': './data/results',
            'visualization': {
                'figsize': [12, 8],
                'dpi': 300
            }
        },
        'logging': {
            'level': 'INFO',
            'files': {
                'main_log': './logs/experiment.log'
            }
        }
    }
    
    # é€’å½’åˆå¹¶é…ç½®
    def merge_dict(base, override):
        for key, value in override.items():
            if key in base and isinstance(base[key], dict) and isinstance(value, dict):
                merge_dict(base[key], value)
            else:
                base[key] = value
    
    merge_dict(default_config, config)
    return default_config

# ============================================================================
# ğŸš€ æ–°å¢ï¼šæ€§èƒ½ç›‘æ§å’Œèµ„æºç®¡ç†
# ============================================================================

class PerformanceMonitor:
    """å¢å¼ºçš„æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self, config: Dict = None):
        self.config = config.get('logging', {}).get('monitoring', {}) if config else {}
        self.enabled = self.config.get('enabled', False)
        self.metrics = deque(maxlen=1000)  # ä¿ç•™æœ€è¿‘1000æ¡è®°å½•
        self.start_time = time.time()
        self.alerts = []
        self._monitoring = False
        self._monitor_thread = None
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§"""
        if not self.enabled or self._monitoring:
            return
            
        self._monitoring = True
        self._monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self._monitor_thread.start()
        logger.info("æ€§èƒ½ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self._monitoring = False
        if self._monitor_thread:
            self._monitor_thread.join(timeout=5)
        logger.info("æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self._monitoring:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                metrics = {
                    'timestamp': time.time(),
                    'memory_usage': psutil.virtual_memory().percent / 100,
                    'cpu_usage': psutil.cpu_percent(interval=1) / 100,
                    'disk_usage': psutil.disk_usage('.').percent / 100,
                    'process_memory': psutil.Process().memory_info().rss / (1024**3)  # GB
                }
                
                self.metrics.append(metrics)
                self._check_alerts(metrics)
                
                time.sleep(30)  # æ¯30ç§’ç›‘æ§ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"ç›‘æ§é”™è¯¯: {e}")
                time.sleep(60)  # å‡ºé”™åç­‰å¾…æ›´é•¿æ—¶é—´
    
    def _check_alerts(self, metrics: Dict):
        """æ£€æŸ¥å‘Šè­¦"""
        alerts = self.config.get('alerts', {})
        
        # å†…å­˜å‘Šè­¦
        if metrics['memory_usage'] > alerts.get('memory_threshold', 0.8):
            alert = f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {metrics['memory_usage']:.1%}"
            if alert not in self.alerts:
                self.alerts.append(alert)
                logger.warning(alert)
        
        # CPUå‘Šè­¦
        if metrics['cpu_usage'] > alerts.get('cpu_threshold', 0.9):
            alert = f"CPUä½¿ç”¨ç‡è¿‡é«˜: {metrics['cpu_usage']:.1%}"
            if alert not in self.alerts:
                self.alerts.append(alert)
                logger.warning(alert)
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.metrics:
            return {}
        
        recent_metrics = list(self.metrics)[-10:]  # æœ€è¿‘10æ¡è®°å½•
        
        return {
            'avg_memory_usage': np.mean([m['memory_usage'] for m in recent_metrics]),
            'avg_cpu_usage': np.mean([m['cpu_usage'] for m in recent_metrics]),
            'peak_memory': max([m['memory_usage'] for m in recent_metrics]),
            'alerts_count': len(self.alerts),
            'uptime': time.time() - self.start_time
        }

class CacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict = None):
        self.config = config.get('resource_management', {}).get('cache', {}) if config else {}
        self.enabled = self.config.get('enabled', True)
        self.max_size = self._parse_size(self.config.get('max_size', '1GB'))
        self.cache_dir = Path(self.config.get('directory', './cache'))
        self.ttl = self._parse_duration(self.config.get('ttl', '7d'))
        
        if self.enabled:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            self._init_cache_db()
    
    def _parse_size(self, size_str: str) -> int:
        """è§£æå¤§å°å­—ç¬¦ä¸²ä¸ºå­—èŠ‚æ•°"""
        units = {'B': 1, 'KB': 1024, 'MB': 1024**2, 'GB': 1024**3}
        if isinstance(size_str, int):
            return size_str
        
        size_str = size_str.upper().strip()
        for unit, multiplier in units.items():
            if size_str.endswith(unit):
                return int(float(size_str[:-len(unit)]) * multiplier)
        return int(size_str)
    
    def _parse_duration(self, duration_str: str) -> int:
        """è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºç§’æ•°"""
        units = {'s': 1, 'm': 60, 'h': 3600, 'd': 86400}
        if isinstance(duration_str, int):
            return duration_str
        
        duration_str = duration_str.lower().strip()
        for unit, multiplier in units.items():
            if duration_str.endswith(unit):
                return int(float(duration_str[:-1]) * multiplier)
        return int(duration_str)
    
    def _init_cache_db(self):
        """åˆå§‹åŒ–ç¼“å­˜æ•°æ®åº“"""
        self.db_path = self.cache_dir / 'cache.db'
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS cache_entries (
                    key TEXT PRIMARY KEY,
                    file_path TEXT,
                    created_at REAL,
                    accessed_at REAL,
                    size INTEGER
                )
            ''')
            conn.commit()
    
    def get(self, key: str):
        """è·å–ç¼“å­˜"""
        if not self.enabled:
            return None
        
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                cursor = conn.execute(
                    'SELECT file_path, created_at FROM cache_entries WHERE key = ?',
                    (key,)
                )
                row = cursor.fetchone()
                
                if row:
                    file_path, created_at = row
                    
                    # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                    if time.time() - created_at > self.ttl:
                        self.delete(key)
                        return None
                    
                    # æ›´æ–°è®¿é—®æ—¶é—´
                    conn.execute(
                        'UPDATE cache_entries SET accessed_at = ? WHERE key = ?',
                        (time.time(), key)
                    )
                    conn.commit()
                    
                    # è¯»å–ç¼“å­˜æ–‡ä»¶
                    cache_file = Path(file_path)
                    if cache_file.exists():
                        if self.config.get('compression', True):
                            with gzip.open(cache_file, 'rb') as f:
                                return pickle.load(f)
                        else:
                            with open(cache_file, 'rb') as f:
                                return pickle.load(f)
        
        except Exception as e:
            logger.debug(f"ç¼“å­˜è¯»å–å¤±è´¥ {key}: {e}")
        
        return None
    
    def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜"""
        if not self.enabled:
            return
        
        try:
            # ç”Ÿæˆç¼“å­˜æ–‡ä»¶è·¯å¾„
            cache_file = self.cache_dir / f"{hashlib.md5(key.encode()).hexdigest()}.cache"
            
            # ä¿å­˜æ•°æ®
            if self.config.get('compression', True):
                with gzip.open(cache_file, 'wb') as f:
                    pickle.dump(value, f)
            else:
                with open(cache_file, 'wb') as f:
                    pickle.dump(value, f)
            
            file_size = cache_file.stat().st_size
            current_time = time.time()
            
            # æ›´æ–°æ•°æ®åº“
            with sqlite3.connect(str(self.db_path)) as conn:
                conn.execute('''
                    INSERT OR REPLACE INTO cache_entries 
                    (key, file_path, created_at, accessed_at, size)
                    VALUES (?, ?, ?, ?, ?)
                ''', (key, str(cache_file), current_time, current_time, file_size))
                conn.commit()
            
            # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
            self._cleanup_if_needed()
        
        except Exception as e:
            logger.debug(f"ç¼“å­˜ä¿å­˜å¤±è´¥ {key}: {e}")
    
    def delete(self, key: str):
        """åˆ é™¤ç¼“å­˜"""
        if not self.enabled:
            return
        
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                cursor = conn.execute(
                    'SELECT file_path FROM cache_entries WHERE key = ?',
                    (key,)
                )
                row = cursor.fetchone()
                
                if row:
                    file_path = Path(row[0])
                    if file_path.exists():
                        file_path.unlink()
                    
                    conn.execute('DELETE FROM cache_entries WHERE key = ?', (key,))
                    conn.commit()
        
        except Exception as e:
            logger.debug(f"ç¼“å­˜åˆ é™¤å¤±è´¥ {key}: {e}")
    
    def _cleanup_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œæ¸…ç†ç¼“å­˜"""
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                # è·å–æ€»å¤§å°
                cursor = conn.execute('SELECT SUM(size) FROM cache_entries')
                total_size = cursor.fetchone()[0] or 0
                
                if total_size > self.max_size:
                    # æŒ‰LRUç­–ç•¥åˆ é™¤
                    cursor = conn.execute('''
                        SELECT key, file_path FROM cache_entries 
                        ORDER BY accessed_at ASC
                    ''')
                    
                    for key, file_path in cursor.fetchall():
                        self.delete(key)
                        
                        # é‡æ–°æ£€æŸ¥å¤§å°
                        cursor2 = conn.execute('SELECT SUM(size) FROM cache_entries')
                        current_size = cursor2.fetchone()[0] or 0
                        
                        if current_size <= self.max_size * 0.8:  # æ¸…ç†åˆ°80%
                            break
        
        except Exception as e:
            logger.debug(f"ç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
    
    def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        if not self.enabled:
            return
        
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                cursor = conn.execute('SELECT file_path FROM cache_entries')
                for (file_path,) in cursor.fetchall():
                    cache_file = Path(file_path)
                    if cache_file.exists():
                        cache_file.unlink()
                
                conn.execute('DELETE FROM cache_entries')
                conn.commit()
            
            logger.info("ç¼“å­˜å·²æ¸…ç©º")
        
        except Exception as e:
            logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
    
    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        if not self.enabled:
            return {'enabled': False}
        
        try:
            with sqlite3.connect(str(self.db_path)) as conn:
                cursor = conn.execute('''
                    SELECT COUNT(*), SUM(size), MAX(accessed_at), MIN(created_at)
                    FROM cache_entries
                ''')
                row = cursor.fetchone()
                
                if row and row[0]:
                    count, total_size, last_access, first_created = row
                    return {
                        'enabled': True,
                        'entry_count': count,
                        'total_size': total_size,
                        'total_size_formatted': format_file_size(total_size),
                        'last_access': datetime.fromtimestamp(last_access).isoformat() if last_access else None,
                        'oldest_entry': datetime.fromtimestamp(first_created).isoformat() if first_created else None,
                        'hit_rate': getattr(self, '_hit_count', 0) / max(getattr(self, '_total_requests', 1), 1)
                    }
        
        except Exception as e:
            logger.debug(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
        
        return {'enabled': True, 'entry_count': 0, 'total_size': 0}

class ResourceMonitor:
    """èµ„æºç›‘æ§å™¨"""
    
    def __init__(self, config: Dict = None):
        self.config = config.get('resource_management', {}) if config else {}
        self.limits = {
            'max_memory': self._parse_size(self.config.get('memory', {}).get('max_usage', '8GB')),
            'max_cpu_cores': self.config.get('cpu', {}).get('max_cores', 4)
        }
        self.warnings_sent = set()
    
    def _parse_size(self, size_str: str) -> int:
        """è§£æå¤§å°å­—ç¬¦ä¸²"""
        units = {'B': 1, 'KB': 1024, 'MB': 1024**2, 'GB': 1024**3}
        if isinstance(size_str, int):
            return size_str
        
        size_str = size_str.upper().strip()
        for unit, multiplier in units.items():
            if size_str.endswith(unit):
                return int(float(size_str[:-len(unit)]) * multiplier)
        return int(size_str)
    
    def check_memory_usage(self) -> bool:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        current_usage = psutil.virtual_memory().used
        
        if current_usage > self.limits['max_memory']:
            warning_key = f"memory_{int(time.time() // 300)}"  # æ¯5åˆ†é’Ÿæœ€å¤šè­¦å‘Šä¸€æ¬¡
            if warning_key not in self.warnings_sent:
                logger.warning(f"å†…å­˜ä½¿ç”¨è¶…é™: {format_file_size(current_usage)} > {format_file_size(self.limits['max_memory'])}")
                self.warnings_sent.add(warning_key)
            return False
        
        return True
    
    def get_available_cores(self) -> int:
        """è·å–å¯ç”¨CPUæ ¸å¿ƒæ•°"""
        return min(psutil.cpu_count(), self.limits['max_cpu_cores'])
    
    def get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('.')
        
        return {
            'cpu_cores': psutil.cpu_count(),
            'cpu_usage': psutil.cpu_percent(interval=1),
            'memory_total': memory.total,
            'memory_used': memory.used,
            'memory_available': memory.available,
            'memory_percent': memory.percent,
            'disk_total': disk.total,
            'disk_used': disk.used,
            'disk_free': disk.free,
            'disk_percent': (disk.used / disk.total) * 100
        }

# ============================================================================
# ğŸ¯ æ–°å¢ï¼šæ¨¡å‹ç®¡ç†ç›¸å…³å·¥å…·
# ============================================================================

def check_model_availability(model_name: str, model_type: str = "huggingface") -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨"""
    try:
        if model_type == "huggingface":
            from transformers import AutoConfig
            # å°è¯•åŠ è½½é…ç½®æ–‡ä»¶ï¼Œä¸ä¸‹è½½æ¨¡å‹
            AutoConfig.from_pretrained(model_name, trust_remote_code=False)
            return True
        
        elif model_type == "local":
            return os.path.exists(model_name)
        
        elif model_type == "textblob":
            try:
                from textblob import TextBlob
                return True
            except ImportError:
                return False
        
        elif model_type == "vader":
            try:
                from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
                return True
            except ImportError:
                return False
    
    except Exception as e:
        logger.debug(f"æ¨¡å‹ {model_name} ä¸å¯ç”¨: {e}")
        return False

def get_model_cache_path(model_name: str) -> Optional[str]:
    """è·å–æ¨¡å‹ç¼“å­˜è·¯å¾„"""
    try:
        from transformers import AutoConfig
        config = AutoConfig.from_pretrained(model_name)
        
        # å°è¯•å¸¸è§çš„ç¼“å­˜ç›®å½•
        cache_dirs = [
            os.path.expanduser("~/.cache/huggingface/transformers"),
            os.path.expanduser("~/.cache/huggingface/hub"),
            "./models"
        ]
        
        for cache_dir in cache_dirs:
            if os.path.exists(cache_dir):
                # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
                for root, dirs, files in os.walk(cache_dir):
                    if any(model_name.replace('/', '--') in d for d in dirs):
                        return root
                    if 'config.json' in files:
                        with open(os.path.join(root, 'config.json'), 'r') as f:
                            cached_config = json.load(f)
                            if cached_config.get('_name_or_path') == model_name:
                                return root
    
    except Exception as e:
        logger.debug(f"è·å–æ¨¡å‹ç¼“å­˜è·¯å¾„å¤±è´¥: {e}")
    
    return None

def download_and_cache_model(model_name: str, cache_dir: str = "./models") -> bool:
    """ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹"""
    try:
        ensure_dir(cache_dir)
        
        from transformers import AutoTokenizer, AutoModel
        
        logger.info(f"ä¸‹è½½æ¨¡å‹: {model_name}")
        
        # ä¸‹è½½åˆ°æŒ‡å®šç›®å½•
        model_path = os.path.join(cache_dir, model_name.replace('/', '--'))
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        
        tokenizer.save_pretrained(model_path)
        model.save_pretrained(model_path)
        
        logger.info(f"æ¨¡å‹å·²ç¼“å­˜åˆ°: {model_path}")
        return True
    
    except Exception as e:
        logger.error(f"ä¸‹è½½æ¨¡å‹å¤±è´¥: {e}")
        return False

# ============================================================================
# ğŸ” æ–°å¢ï¼šæ™ºèƒ½æ–‡ä»¶å¤„ç†
# ============================================================================

def smart_file_validator(file_path: str, config: Dict = None) -> Dict[str, Any]:
    """æ™ºèƒ½æ–‡ä»¶éªŒè¯å™¨"""
    result = {
        'is_valid': False,
        'file_type': 'unknown',
        'size': 0,
        'issues': [],
        'quality_score': 0.0,
        'metadata': {}
    }
    
    try:
        if not os.path.exists(file_path):
            result['issues'].append('æ–‡ä»¶ä¸å­˜åœ¨')
            return result
        
        file_size = os.path.getsize(file_path)
        result['size'] = file_size
        
        # åŸºäºé…ç½®çš„è´¨é‡æ£€æŸ¥
        quality_config = config.get('data_collection', {}).get('quality_control', {}) if config else {}
        
        # æ–‡ä»¶å¤§å°æ£€æŸ¥
        min_size = quality_config.get('min_file_size', 50000)
        max_size = quality_config.get('max_file_size', 10485760)
        
        if file_size < min_size:
            result['issues'].append(f'æ–‡ä»¶è¿‡å°: {format_file_size(file_size)} < {format_file_size(min_size)}')
        elif file_size > max_size:
            result['issues'].append(f'æ–‡ä»¶è¿‡å¤§: {format_file_size(file_size)} > {format_file_size(max_size)}')
        
        # PDFç‰¹å®šæ£€æŸ¥
        if file_path.lower().endswith('.pdf'):
            result['file_type'] = 'pdf'
            pdf_info = get_pdf_info(file_path)
            result['metadata'] = pdf_info
            
            if pdf_info['is_valid']:
                result['is_valid'] = True
                
                # é¡µæ•°æ£€æŸ¥
                min_pages = quality_config.get('min_pages', 4)
                max_pages = quality_config.get('max_pages', 50)
                page_count = pdf_info['page_count']
                
                if page_count < min_pages:
                    result['issues'].append(f'é¡µæ•°è¿‡å°‘: {page_count} < {min_pages}')
                elif page_count > max_pages:
                    result['issues'].append(f'é¡µæ•°è¿‡å¤š: {page_count} > {max_pages}')
                
                # æ–‡æœ¬å†…å®¹æ£€æŸ¥
                if not pdf_info['has_text']:
                    result['issues'].append('ç¼ºå°‘æ–‡æœ¬å†…å®¹')
                
                # è®¡ç®—è´¨é‡åˆ†æ•°
                score = 1.0
                score -= len(result['issues']) * 0.2  # æ¯ä¸ªé—®é¢˜æ‰£0.2åˆ†
                score = max(0.0, min(1.0, score))
                result['quality_score'] = score
            
            else:
                result['issues'].append('PDFæ–‡ä»¶æŸåæˆ–æ— æ•ˆ')
        
        # å…¶ä»–æ–‡ä»¶ç±»å‹çš„æ£€æŸ¥å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
        
    except Exception as e:
        result['issues'].append(f'éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}')
        logger.debug(f"æ–‡ä»¶éªŒè¯å¤±è´¥ {file_path}: {e}")
    
    return result

def batch_file_processor(file_paths: List[str], 
                        processor_func: callable,
                        max_workers: int = 4,
                        progress_desc: str = "å¤„ç†æ–‡ä»¶") -> List[Any]:
    """æ‰¹é‡æ–‡ä»¶å¤„ç†å™¨"""
    results = []
    
    if not file_paths:
        return results
    
    # å•çº¿ç¨‹å¤„ç†ï¼ˆé¿å…å¤æ‚æ€§ï¼‰
    with ProgressTracker(len(file_paths), progress_desc) as progress:
        for file_path in file_paths:
            try:
                result = processor_func(file_path)
                results.append(result)
            except Exception as e:
                logger.debug(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                results.append(None)
            
            progress.update()
    
    return results

# ============================================================================
# ğŸ›ï¸ æ–°å¢ï¼šé…ç½®å·¥å…·
# ============================================================================

def create_default_config() -> Dict[str, Any]:
    """åˆ›å»ºé»˜è®¤é…ç½®"""
    return {
        'data_collection': {
            'download_dir': './data/clean_papers',
            'max_papers': 300,
            'quality_control': {
                'min_file_size': 50000,
                'max_file_size': 10485760,
                'min_pages': 4,
                'max_pages': 50
            }
        },
        'detection': {
            'offline_mode': True,
            'thresholds': {
                'risk_score': 0.35,
                'detection_count': 2
            },
            'detection_weights': {
                'semantic_injection': 1.8,
                'small_text_injection': 0.4
            }
        },
        'experiment': {
            'output_dir': './data/results'
        },
        'logging': {
            'level': 'INFO',
            'files': {
                'main_log': './logs/experiment.log'
            }
        }
    }

def merge_configs(base_config: Dict, override_config: Dict) -> Dict:
    """åˆå¹¶é…ç½®"""
    def _merge_dict(base, override):
        result = base.copy()
        for key, value in override.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = _merge_dict(result[key], value)
            else:
                result[key] = value
        return result
    
    return _merge_dict(base_config, override_config)

# ============================================================================
# ä¿æŒä½ çš„åŸæœ‰å‡½æ•°ï¼ˆensure_dir, calculate_file_hash ç­‰ï¼‰
# ============================================================================

def ensure_dir(dir_path: str) -> str:
    """ç¡®ä¿ç›®å½•å­˜åœ¨"""
    Path(dir_path).mkdir(parents=True, exist_ok=True)
    return dir_path

def calculate_file_hash(file_path: str) -> str:
    """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ"""
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
        return ""

def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬"""
    if not text:
        return ""
    
    # ç§»é™¤å¤šä½™ç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()-]', '', text)
    return text.strip()

def extract_metadata_info(metadata: Dict) -> Dict:
    """æå–æœ‰ç”¨çš„å…ƒæ•°æ®ä¿¡æ¯"""
    useful_fields = ['title', 'author', 'subject', 'keywords', 'creator', 'producer']
    result = {}
    
    for field in useful_fields:
        if field in metadata and metadata[field]:
            result[field] = str(metadata[field])
    
    return result

def detect_language(text: str) -> str:
    """ç®€å•çš„è¯­è¨€æ£€æµ‹"""
    if not text:
        return "unknown"
    
    # ä¸­æ–‡å­—ç¬¦
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    # æ—¥æ–‡å­—ç¬¦
    japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff]', text))
    # è‹±æ–‡å­—ç¬¦
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    
    total_chars = chinese_chars + japanese_chars + english_chars
    
    if total_chars == 0:
        return "unknown"
    
    chinese_ratio = chinese_chars / total_chars
    japanese_ratio = japanese_chars / total_chars
    english_ratio = english_chars / total_chars
    
    if chinese_ratio > 0.3:
        return "chinese"
    elif japanese_ratio > 0.2:
        return "japanese"
    elif english_ratio > 0.7:
        return "english"
    else:
        return "mixed"

def save_results(results: Dict, output_path: str):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    try:
        ensure_dir(os.path.dirname(output_path))
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")

def load_results(input_path: str) -> Dict:
    """ä»JSONæ–‡ä»¶åŠ è½½ç»“æœ"""
    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            results = json.load(f)
        logger.info(f"ç»“æœå·²åŠ è½½: {input_path}")
        return results
    except Exception as e:
        logger.error(f"åŠ è½½ç»“æœå¤±è´¥: {e}")
        return {}

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨ - æ”¹è¿›ç‰ˆ"""
    
    def __init__(self, total: int, description: str = "Processing"):
        self.total = total
        self.current = 0
        self.description = description
        self.start_time = datetime.now()
        self.last_update_time = datetime.now()
        self.update_interval = 1.0  # æœ€å°æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
    
    def update(self, step: int = 1):
        """æ›´æ–°è¿›åº¦"""
        self.current += step
        
        # é™åˆ¶æ›´æ–°é¢‘ç‡
        now = datetime.now()
        if (now - self.last_update_time).total_seconds() < self.update_interval and self.current < self.total:
            return
        
        self.last_update_time = now
        percentage = (self.current / self.total) * 100
        elapsed = now - self.start_time
        
        if self.current > 0:
            eta = elapsed * (self.total - self.current) / self.current
            eta_str = str(eta).split('.')[0]  # å»æ‰å¾®ç§’
            logger.info(f"{self.description}: {self.current}/{self.total} ({percentage:.1f}%) - ETA: {eta_str}")
        else:
            logger.info(f"{self.description}: {self.current}/{self.total} ({percentage:.1f}%)")
    
    def finish(self):
        """å®Œæˆè¿›åº¦"""
        elapsed = datetime.now() - self.start_time
        elapsed_str = str(elapsed).split('.')[0]  # å»æ‰å¾®ç§’
        logger.info(f"{self.description} å®Œæˆ! ç”¨æ—¶: {elapsed_str}")
    
    def __enter__(self):
        """æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """è‡ªåŠ¨å®Œæˆ"""
        self.finish()

# ============================================================================
# ä¿æŒä½ çš„åŸæœ‰PDFå¤„ç†å‡½æ•°
# ============================================================================

def configure_pdf_error_suppression():
    """é…ç½®PDFé”™è¯¯æŠ‘åˆ¶"""
    warnings.filterwarnings("ignore", category=UserWarning)
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    
    try:
        if hasattr(fitz, 'TOOLS'):
            fitz.TOOLS.mupdf_display_errors(False)
    except (AttributeError, Exception):
        pass
    
    os.environ['MUPDF_DISPLAY_ERRORS'] = '0'

def safe_pdf_operation(func):
    """PDFæ“ä½œçš„å®‰å…¨è£…é¥°å™¨"""
    def wrapper(*args, **kwargs):
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                return func(*args, **kwargs)
        except Exception as e:
            logger.debug(f"PDFæ“ä½œå¤±è´¥: {e}")
            return None
    return wrapper

@safe_pdf_operation
def validate_pdf(file_path: str, repair_if_needed: bool = True) -> bool:
    """å¢å¼ºçš„PDFéªŒè¯å‡½æ•°"""
    if not os.path.exists(file_path):
        return False
    
    if os.path.getsize(file_path) == 0:
        return False
    
    try:
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            
            doc = fitz.open(file_path)
            
            if doc.page_count == 0:
                doc.close()
                return False
            
            try:
                page = doc[0]
                rect = page.rect
                if rect.width <= 0 or rect.height <= 0:
                    doc.close()
                    return False
                
                try:
                    text = page.get_text()[:100]
                except:
                    pass
                    
            except Exception as e:
                logger.debug(f"PDFé¡µé¢è®¿é—®å¤±è´¥ {file_path}: {e}")
                doc.close()
                return False
            
            doc.close()
            return True
            
    except Exception as e:
        logger.debug(f"PDFéªŒè¯å¤±è´¥ {file_path}: {e}")
        
        if repair_if_needed:
            return _try_repair_pdf(file_path)
        
        return False

@safe_pdf_operation
def _try_repair_pdf(file_path: str) -> bool:
    """å°è¯•ä¿®å¤PDFæ–‡ä»¶"""
    try:
        temp_path = file_path + ".repaired.tmp"
        
        doc = fitz.open(file_path)
        
        doc.save(
            temp_path, 
            garbage=4,
            deflate=True,
            clean=True,
            ascii=False,
            linear=False,
            pretty=False,
            encryption=fitz.PDF_ENCRYPT_NONE
        )
        doc.close()
        
        if validate_pdf(temp_path, repair_if_needed=False):
            import shutil
            shutil.move(temp_path, file_path)
            logger.info(f"PDFä¿®å¤æˆåŠŸ: {file_path}")
            return True
        else:
            if os.path.exists(temp_path):
                os.remove(temp_path)
            return False
            
    except Exception as e:
        logger.debug(f"PDFä¿®å¤å¤±è´¥ {file_path}: {e}")
        return False

@safe_pdf_operation
def get_pdf_info(file_path: str) -> Dict[str, Any]:
    """å®‰å…¨åœ°è·å–PDFåŸºæœ¬ä¿¡æ¯"""
    info = {
        'page_count': 0,
        'file_size': 0,
        'is_valid': False,
        'has_text': False,
        'metadata': {},
        'error': None
    }
    
    try:
        info['file_size'] = os.path.getsize(file_path)
        
        if not validate_pdf(file_path):
            info['error'] = "PDFæ–‡ä»¶æ— æ•ˆ"
            return info
        
        doc = fitz.open(file_path)
        info['page_count'] = doc.page_count
        info['is_valid'] = True
        
        try:
            info['metadata'] = doc.metadata or {}
        except:
            info['metadata'] = {}
        
        try:
            if doc.page_count > 0:
                first_page = doc[0]
                sample_text = first_page.get_text()[:500]
                info['has_text'] = len(sample_text.strip()) > 0
        except:
            info['has_text'] = False
        
        doc.close()
        
    except Exception as e:
        info['error'] = str(e)
        logger.debug(f"è·å–PDFä¿¡æ¯å¤±è´¥ {file_path}: {e}")
    
    return info

def get_file_size(file_path: str) -> int:
    """è·å–æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰"""
    try:
        return os.path.getsize(file_path)
    except Exception:
        return 0

def format_file_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
    if size_bytes == 0:
        return "0B"
    
    size_names = ["B", "KB", "MB", "GB"]
    i = 0
    while size_bytes >= 1024 and i < len(size_names) - 1:
        size_bytes /= 1024.0
        i += 1
    
    return f"{size_bytes:.1f}{size_names[i]}"

def scan_pdf_files(directory: str, recursive: bool = True, 
                   validate_files: bool = True) -> List[Dict[str, Any]]:
    """æ‰«æç›®å½•ä¸­çš„PDFæ–‡ä»¶"""
    pdf_files = []
    
    try:
        directory_path = Path(directory)
        if not directory_path.exists():
            logger.error(f"ç›®å½•ä¸å­˜åœ¨: {directory}")
            return pdf_files
        
        pattern = "**/*.pdf" if recursive else "*.pdf"
        pdf_paths = list(directory_path.glob(pattern))
        
        if not pdf_paths:
            logger.warning(f"åœ¨ç›®å½•ä¸­æœªæ‰¾åˆ°PDFæ–‡ä»¶: {directory}")
            return pdf_files
        
        logger.info(f"å‘ç° {len(pdf_paths)} ä¸ªPDFæ–‡ä»¶")
        
        with ProgressTracker(len(pdf_paths), "æ‰«æPDFæ–‡ä»¶") as progress:
            for pdf_path in pdf_paths:
                try:
                    file_info = {
                        'path': str(pdf_path),
                        'name': pdf_path.name,
                        'size': pdf_path.stat().st_size,
                        'size_formatted': format_file_size(pdf_path.stat().st_size)
                    }
                    
                    if validate_files:
                        pdf_info = get_pdf_info(str(pdf_path))
                        file_info.update(pdf_info)
                        
                        if pdf_info['is_valid']:
                            pdf_files.append(file_info)
                        else:
                            logger.debug(f"è·³è¿‡æ— æ•ˆPDFæ–‡ä»¶: {pdf_path}")
                    else:
                        pdf_files.append(file_info)
                    
                except Exception as e:
                    logger.debug(f"å¤„ç†PDFæ–‡ä»¶å¤±è´¥ {pdf_path}: {e}")
                
                progress.update()
        
        logger.info(f"æˆåŠŸæ‰«æ {len(pdf_files)} ä¸ªæœ‰æ•ˆPDFæ–‡ä»¶")
        
    except Exception as e:
        logger.error(f"æ‰«æPDFæ–‡ä»¶å¤±è´¥: {e}")
    
    return pdf_files

def batch_validate_pdfs(file_paths: List[str], repair_errors: bool = False) -> Dict[str, Any]:
    """æ‰¹é‡éªŒè¯PDFæ–‡ä»¶"""
    results = {
        'total_files': len(file_paths),
        'valid_files': [],
        'invalid_files': [],
        'repaired_files': [],
        'errors': []
    }
    
    if not file_paths:
        return results
    
    with ProgressTracker(len(file_paths), "éªŒè¯PDFæ–‡ä»¶") as progress:
        for file_path in file_paths:
            try:
                is_valid = validate_pdf(file_path, repair_if_needed=repair_errors)
                
                if is_valid:
                    results['valid_files'].append(file_path)
                else:
                    results['invalid_files'].append(file_path)
                    
                    if repair_errors:
                        if _try_repair_pdf(file_path):
                            results['repaired_files'].append(file_path)
                            results['valid_files'].append(file_path)
                            results['invalid_files'].remove(file_path)
                
            except Exception as e:
                error_info = {'file': file_path, 'error': str(e)}
                results['errors'].append(error_info)
                logger.debug(f"éªŒè¯PDFå¤±è´¥ {file_path}: {e}")
            
            progress.update()
    
    logger.info(f"PDFéªŒè¯å®Œæˆ:")
    logger.info(f"  æ€»æ–‡ä»¶æ•°: {results['total_files']}")
    logger.info(f"  æœ‰æ•ˆæ–‡ä»¶: {len(results['valid_files'])}")
    logger.info(f"  æ— æ•ˆæ–‡ä»¶: {len(results['invalid_files'])}")
    logger.info(f"  ä¿®å¤æ–‡ä»¶: {len(results['repaired_files'])}")
    logger.info(f"  é”™è¯¯æ–‡ä»¶: {len(results['errors'])}")
    
    return results

def create_file_backup(file_path: str, backup_dir: str = None) -> Optional[str]:
    """åˆ›å»ºæ–‡ä»¶å¤‡ä»½"""
    try:
        if backup_dir is None:
            backup_dir = os.path.dirname(file_path)
        
        ensure_dir(backup_dir)
        
        file_name = os.path.basename(file_path)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{timestamp}_{file_name}"
        backup_path = os.path.join(backup_dir, backup_name)
        
        import shutil
        shutil.copy2(file_path, backup_path)
        
        logger.debug(f"æ–‡ä»¶å¤‡ä»½æˆåŠŸ: {file_path} -> {backup_path}")
        return backup_path
        
    except Exception as e:
        logger.error(f"åˆ›å»ºæ–‡ä»¶å¤‡ä»½å¤±è´¥ {file_path}: {e}")
        return None

# ============================================================================
# ğŸš€ æ–°å¢ï¼šå…¨å±€ç®¡ç†å™¨
# ============================================================================

class GlobalManager:
    """å…¨å±€ç®¡ç†å™¨ï¼Œæ•´åˆæ‰€æœ‰ç»„ä»¶"""
    
    _instance = None
    
    def __new__(cls, config: Dict = None):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, config: Dict = None):
        if self._initialized:
            return
        
        self.config = config or load_config()
        self.performance_monitor = PerformanceMonitor(self.config)
        self.cache_manager = CacheManager(self.config)
        self.resource_monitor = ResourceMonitor(self.config)
        
        # å¯åŠ¨ç›‘æ§
        if self.config.get('logging', {}).get('monitoring', {}).get('enabled', False):
            self.performance_monitor.start_monitoring()
        
        self._initialized = True
        logger.info("å…¨å±€ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'performance_monitor'):
            self.performance_monitor.stop_monitoring()
        logger.info("å…¨å±€ç®¡ç†å™¨æ¸…ç†å®Œæˆ")
    
    def get_stats(self) -> Dict:
        """è·å–æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'performance': self.performance_monitor.get_stats(),
            'cache': self.cache_manager.get_stats(),
            'system': self.resource_monitor.get_system_info()
        }

# åˆå§‹åŒ–
configure_pdf_error_suppression()

# å¯¼å‡ºä¸»è¦ç±»å’Œå‡½æ•°
__all__ = [
    'setup_logging', 'load_config', 'ensure_dir', 'calculate_file_hash',
    'clean_text', 'detect_language', 'save_results', 'load_results',
    'ProgressTracker', 'validate_pdf', 'get_pdf_info', 'scan_pdf_files',
    'PerformanceMonitor', 'CacheManager', 'ResourceMonitor', 'GlobalManager',
    'smart_file_validator', 'batch_file_processor', 'check_model_availability'
]
